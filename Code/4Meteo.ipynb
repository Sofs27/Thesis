{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0159eacb",
   "metadata": {},
   "source": [
    "Precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109258f",
   "metadata": {},
   "source": [
    "Daily Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11735df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Define the folder containing NetCDF files\n",
    "folder_path = r\"D:\\IPMA\\ERA5\\Precipitation\\1raw_year_1979_2024\"\n",
    "output_folder = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get a sorted list of all NetCDF files in the folder\n",
    "file_list = sorted([f for f in os.listdir(folder_path) if f.endswith(\".nc\")])\n",
    "\n",
    "# Filter only years 2003â€“2024\n",
    "file_list = [f for f in file_list if 2003 <= int(f.split('_')[-1][:4]) <= 2024]\n",
    "\n",
    "print(f\"Processing {len(file_list)} files from 2003 to 2024...\")\n",
    "\n",
    "# Loop over files with a progress bar\n",
    "for file in tqdm(file_list, desc=\"Files processed\"):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Extract year from filename\n",
    "    year_val = int(file.split('_')[-1][:4])\n",
    "    print(f\"\\nðŸ“‚ Processing year {year_val}...\")\n",
    "    \n",
    "    ds = xr.open_dataset(file_path)  \n",
    "    ds = ds.chunk({'valid_time': 500})  \n",
    "    \n",
    "    # Add year, month, day as coordinates\n",
    "    ds = ds.assign_coords(\n",
    "        year=ds['valid_time'].dt.year,\n",
    "        month=ds['valid_time'].dt.month,\n",
    "        day=ds['valid_time'].dt.day\n",
    "    )\n",
    "    \n",
    "    # Temporary list for this year\n",
    "    df_list = []\n",
    "    \n",
    "    # Loop over each unique month in this file\n",
    "    for month_val in np.unique(ds['month'].values):\n",
    "        days_in_month = np.unique(ds['day'].values[ds['month'].values == month_val])\n",
    "        \n",
    "        # Loop over each day in the month with a progress bar\n",
    "        for day_val in tqdm(days_in_month, desc=f\"Month {month_val}\", leave=False):\n",
    "            ds_day = ds.sel(\n",
    "                valid_time=(ds['month'] == month_val) & (ds['day'] == day_val)\n",
    "            )\n",
    "            if ds_day['valid_time'].size == 0:\n",
    "                continue\n",
    "            \n",
    "            precip_data = ds_day['tp'].values\n",
    "\n",
    "            # Compute daily statistics\n",
    "            mean = np.nanmean(precip_data, axis=0)\n",
    "            median = np.nanmedian(precip_data, axis=0)\n",
    "            std = np.nanstd(precip_data, axis=0)\n",
    "            max_ = np.nanmax(precip_data, axis=0)\n",
    "            min_ = np.nanmin(precip_data, axis=0)\n",
    "            total = np.nansum(precip_data, axis=0)\n",
    "\n",
    "            # Create Dataset for this day's stats\n",
    "            stats = xr.Dataset({\n",
    "                'Mean': (['latitude', 'longitude'], mean),\n",
    "                'Median': (['latitude', 'longitude'], median),\n",
    "                'Std': (['latitude', 'longitude'], std),\n",
    "                'Max': (['latitude', 'longitude'], max_),\n",
    "                'Min': (['latitude', 'longitude'], min_),\n",
    "                'Total_Precipitation': (['latitude', 'longitude'], total)\n",
    "            }, coords={'latitude': ds['latitude'], 'longitude': ds['longitude']})\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            stats_df = stats.to_dataframe().reset_index()\n",
    "\n",
    "            # Add time labels\n",
    "            stats_df['Year'] = year_val\n",
    "            stats_df['Month'] = month_val\n",
    "            stats_df['Day'] = int(day_val)\n",
    "\n",
    "            # Set multi-index\n",
    "            stats_df = stats_df.set_index(['Year', 'Month', 'Day', 'latitude', 'longitude'])\n",
    "            df_list.append(stats_df)\n",
    "    \n",
    "    # Save this year's results\n",
    "    if df_list:  # Only save if data exists\n",
    "        df_final = pd.concat(df_list)\n",
    "        df_final_xr = df_final.reset_index().set_index(['Year', 'Month', 'Day', 'latitude', 'longitude'])\n",
    "        df_final_xr = df_final_xr.to_xarray()\n",
    "        \n",
    "        output_file_path = os.path.join(output_folder, f\"daily_precipitation_stats_{year_val}.nc\")\n",
    "        df_final_xr.to_netcdf(output_file_path)\n",
    "        print(f\"âœ… Saved {output_file_path}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All yearly files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67240427",
   "metadata": {},
   "source": [
    "Convert lat and lon from 0.25 to 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73090de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input/output folders\n",
    "input_folder = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly\"\n",
    "output_folder = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of input NetCDF files\n",
    "file_list = sorted([f for f in os.listdir(input_folder) if f.endswith(\".nc\")])\n",
    "print(f\"Found {len(file_list)} yearly files to regrid...\")\n",
    "\n",
    "# Regrid parameters\n",
    "factor_lat = 3\n",
    "factor_lon = 3\n",
    "\n",
    "# Target grid\n",
    "lat_target = np.arange(34.5, 66.0 + 0.001, 0.75)   # 43 lats\n",
    "lon_target = np.arange(-12.0, 36.0 + 0.001, 0.75)  # 65 lons\n",
    "\n",
    "# Loop through files\n",
    "for file in tqdm(file_list, desc=\"Regridding yearly files\"):\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    year_val = file.split('_')[-1][:4]  # extract year from filename\n",
    "    \n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Step 1: Coarsen (aggregation)\n",
    "    ds_coarse = xr.Dataset()\n",
    "    for var in [\"Mean\", \"Median\", \"Std\", \"Max\", \"Min\", \"Total_Precipitation\"]:\n",
    "        if var in ds:\n",
    "            ds_coarse[var] = ds[var].coarsen(\n",
    "                latitude=factor_lat, longitude=factor_lon, boundary=\"trim\"\n",
    "            ).mean(skipna=True)\n",
    "\n",
    "    # Handle \"number\" only if it's in the dataset AND has lat/lon dims\n",
    "    if \"number\" in ds and {\"latitude\", \"longitude\"}.issubset(ds[\"number\"].dims):\n",
    "        ds_coarse[\"number\"] = ds[\"number\"].coarsen(\n",
    "            latitude=factor_lat, longitude=factor_lon, boundary=\"trim\"\n",
    "        ).sum(skipna=True)\n",
    "\n",
    "\n",
    "    # Copy non-spatial coords (if they align)\n",
    "    for coord in [\"Year\", \"Month\", \"Day\"]:\n",
    "        if coord in ds:\n",
    "            ds_coarse[coord] = ds[coord]\n",
    "    \n",
    "    # Step 2: Interpolate onto target grid\n",
    "    ds_final = ds_coarse.interp(\n",
    "        latitude=lat_target,\n",
    "        longitude=lon_target,\n",
    "        method=\"linear\",\n",
    "        kwargs={\"fill_value\": \"extrapolate\"}\n",
    "    )\n",
    "    \n",
    "    # Save output\n",
    "    out_file = os.path.join(output_folder, f\"daily_precipitation_stats_{year_val}_regrid.nc\")\n",
    "    ds_final.to_netcdf(out_file)\n",
    "    print(f\"âœ… Saved {out_file}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All yearly files regridded and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad16279",
   "metadata": {},
   "source": [
    "Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02140b",
   "metadata": {},
   "source": [
    "Daily Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c565436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Define the folder containing NetCDF files\n",
    "folder_path = r\"D:\\IPMA\\ERA5\\Temperature\\2conversion_year_1979_2024\"\n",
    "output_folder = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get a sorted list of all NetCDF files in the folder\n",
    "file_list = sorted([f for f in os.listdir(folder_path) if f.endswith(\".nc\")])\n",
    "\n",
    "# Filter only years 2003â€“2024\n",
    "file_list = [f for f in file_list if 2003 <= int(f.split('_')[-1][:4]) <= 2024]\n",
    "\n",
    "print(f\"Processing {len(file_list)} files from 2003 to 2024...\")\n",
    "\n",
    "# Loop over files with a progress bar\n",
    "for file in tqdm(file_list, desc=\"Files processed\"):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Extract year from filename\n",
    "    year_val = int(file.split('_')[-1][:4])\n",
    "    print(f\"\\nðŸ“‚ Processing year {year_val}...\")\n",
    "    \n",
    "    ds = xr.open_dataset(file_path)  \n",
    "    ds = ds.chunk({'valid_time': 500})  \n",
    "    \n",
    "    # Add year, month, day as coordinates\n",
    "    ds = ds.assign_coords(\n",
    "        year=ds['valid_time'].dt.year,\n",
    "        month=ds['valid_time'].dt.month,\n",
    "        day=ds['valid_time'].dt.day\n",
    "    )\n",
    "    \n",
    "    # Temporary list for this year\n",
    "    df_list = []\n",
    "    \n",
    "    # Loop over each unique month in this file\n",
    "    for month_val in np.unique(ds['month'].values):\n",
    "        days_in_month = np.unique(ds['day'].values[ds['month'].values == month_val])\n",
    "        \n",
    "        # Loop over each day in the month with a progress bar\n",
    "        for day_val in tqdm(days_in_month, desc=f\"Month {month_val}\", leave=False):\n",
    "            ds_day = ds.sel(\n",
    "                valid_time=(ds['month'] == month_val) & (ds['day'] == day_val)\n",
    "            )\n",
    "            if ds_day['valid_time'].size == 0:\n",
    "                continue\n",
    "            \n",
    "            temp_data = ds_day['t2m'].values\n",
    "\n",
    "            # Compute daily statistics\n",
    "            mean = np.nanmean(temp_data, axis=0)\n",
    "            median = np.nanmedian(temp_data, axis=0)\n",
    "            std = np.nanstd(temp_data, axis=0)\n",
    "            max_ = np.nanmax(temp_data, axis=0)\n",
    "            min_ = np.nanmin(temp_data, axis=0)\n",
    "\n",
    "            # Create Dataset for this day's stats\n",
    "            stats = xr.Dataset({\n",
    "                'Mean': (['latitude', 'longitude'], mean),\n",
    "                'Median': (['latitude', 'longitude'], median),\n",
    "                'Std': (['latitude', 'longitude'], std),\n",
    "                'Max': (['latitude', 'longitude'], max_),\n",
    "                'Min': (['latitude', 'longitude'], min_)\n",
    "            }, coords={'latitude': ds['latitude'], 'longitude': ds['longitude']})\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            stats_df = stats.to_dataframe().reset_index()\n",
    "\n",
    "            # Add time labels\n",
    "            stats_df['Year'] = year_val\n",
    "            stats_df['Month'] = month_val\n",
    "            stats_df['Day'] = int(day_val)\n",
    "\n",
    "            # Set multi-index\n",
    "            stats_df = stats_df.set_index(['Year', 'Month', 'Day', 'latitude', 'longitude'])\n",
    "            df_list.append(stats_df)\n",
    "    \n",
    "    # Save this year's results\n",
    "    if df_list:  # Only save if data exists\n",
    "        df_final = pd.concat(df_list)\n",
    "        df_final_xr = df_final.reset_index().set_index(['Year', 'Month', 'Day', 'latitude', 'longitude'])\n",
    "        df_final_xr = df_final_xr.to_xarray()\n",
    "        \n",
    "        output_file_path = os.path.join(output_folder, f\"daily_temperature_stats_{year_val}.nc\")\n",
    "        df_final_xr.to_netcdf(output_file_path)\n",
    "        print(f\"âœ… Saved {output_file_path}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All yearly files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a6966e",
   "metadata": {},
   "source": [
    "Convert lat and lon from 0.25 to 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input/output folders\n",
    "input_folder = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly\"\n",
    "output_folder = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of input NetCDF files\n",
    "file_list = sorted([f for f in os.listdir(input_folder) if f.endswith(\".nc\")])\n",
    "print(f\"Found {len(file_list)} yearly files to regrid...\")\n",
    "\n",
    "# Regrid parameters\n",
    "factor_lat = 3\n",
    "factor_lon = 3\n",
    "\n",
    "# Target grid\n",
    "lat_target = np.arange(34.5, 66.0 + 0.001, 0.75)   # 43 lats\n",
    "lon_target = np.arange(-12.0, 36.0 + 0.001, 0.75)  # 65 lons\n",
    "\n",
    "# Loop through files\n",
    "for file in tqdm(file_list, desc=\"Regridding yearly files\"):\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    year_val = file.split('_')[-1][:4]  # extract year from filename\n",
    "    \n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Step 1: Coarsen (aggregation)\n",
    "    ds_coarse = xr.Dataset()\n",
    "    for var in [\"Mean\", \"Median\", \"Std\", \"Max\", \"Min\"]:\n",
    "        if var in ds:\n",
    "            ds_coarse[var] = ds[var].coarsen(\n",
    "                latitude=factor_lat, longitude=factor_lon, boundary=\"trim\"\n",
    "            ).mean(skipna=True)\n",
    "\n",
    "    # Handle \"number\" only if it's in the dataset AND has lat/lon dims\n",
    "    if \"number\" in ds and {\"latitude\", \"longitude\"}.issubset(ds[\"number\"].dims):\n",
    "        ds_coarse[\"number\"] = ds[\"number\"].coarsen(\n",
    "            latitude=factor_lat, longitude=factor_lon, boundary=\"trim\"\n",
    "        ).sum(skipna=True)\n",
    "\n",
    "    # Copy non-spatial coords (if they align)\n",
    "    for coord in [\"Year\", \"Month\", \"Day\"]:\n",
    "        if coord in ds:\n",
    "            ds_coarse[coord] = ds[coord]\n",
    "    \n",
    "    # Step 2: Interpolate onto target grid\n",
    "    ds_final = ds_coarse.interp(\n",
    "        latitude=lat_target,\n",
    "        longitude=lon_target,\n",
    "        method=\"linear\",\n",
    "        kwargs={\"fill_value\": \"extrapolate\"}\n",
    "    )\n",
    "    \n",
    "    # Save output\n",
    "    out_file = os.path.join(output_folder, f\"daily_temperature_stats_{year_val}_regrid.nc\")\n",
    "    ds_final.to_netcdf(out_file)\n",
    "    print(f\"âœ… Saved {out_file}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All yearly temperature files regridded and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563134be",
   "metadata": {},
   "source": [
    "Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate wind speed and direction based on u&v component\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define folders\n",
    "input_folder = r\"E:\\IPMA\\ERA5\\UV_wind\\1raw_year_1979_2024\"\n",
    "output_folder = r\"E:\\IPMA\\ERA5\\UV_wind\\2wind_speed_direction\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find all relevant NetCDF files\n",
    "nc_files = sorted(glob(os.path.join(input_folder, \"ERA5_hourly_uv_*.nc\")))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Calculate wind speed\n",
    "    wind_speed = np.sqrt(ds['u10']**2 + ds['v10']**2)\n",
    "\n",
    "    # Calculate wind direction (degrees, meteorological convention)\n",
    "    wind_dir = (180 + np.degrees(np.arctan2(ds['u10'], ds['v10']))) % 360\n",
    "\n",
    "    # Add to dataset\n",
    "    ds = ds.assign(wind_speed=wind_speed, wind_direction=wind_dir)\n",
    "\n",
    "    # Add metadata\n",
    "    ds['wind_speed'].attrs['units'] = 'm/s'\n",
    "    ds['wind_speed'].attrs['description'] = '10m wind speed calculated from u10 and v10'\n",
    "    ds['wind_direction'].attrs['units'] = 'degrees'\n",
    "    ds['wind_direction'].attrs['description'] = 'Wind direction (from which wind blows, 0Â°=North, clockwise)'\n",
    "\n",
    "    # Create output filename, e.g., ERA5_hourly_wind_1979.nc\n",
    "    year_str = os.path.basename(file_path).split('_')[-1].split('.')[0]\n",
    "    out_filename = f\"ERA5_hourly_wind_{year_str}.nc\"\n",
    "    out_path = os.path.join(output_folder, out_filename)\n",
    "\n",
    "    # Save only the wind_speed and wind_direction variables (optional)\n",
    "    ds[['wind_speed', 'wind_direction']].to_netcdf(out_path)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "print(\"âœ… All files processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f116e",
   "metadata": {},
   "source": [
    "Daily statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "# Define the folder containing NetCDF files\n",
    "folder_path = r\"D:\\IPMA\\ERA5\\UV_wind\\2wind_speed_direction\"\n",
    "output_folder = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get a sorted list of all NetCDF files in the folder\n",
    "file_list = sorted([f for f in os.listdir(folder_path) if f.endswith(\".nc\")])\n",
    "\n",
    "# Filter only years 2003â€“2024\n",
    "file_list = [f for f in file_list if 2003 <= int(f.split('_')[-1][:4]) <= 2024]\n",
    "\n",
    "print(f\"Processing {len(file_list)} files from 2003 to 2024...\")\n",
    "\n",
    "# Loop over files with a progress bar\n",
    "for file in tqdm(file_list, desc=\"Files processed\"):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Extract year from filename\n",
    "    year_val = int(file.split('_')[-1][:4])\n",
    "    print(f\"\\nðŸ“‚ Processing year {year_val}...\")\n",
    "    \n",
    "    ds = xr.open_dataset(file_path)  \n",
    "    ds = ds.chunk({'valid_time': 500})  \n",
    "    \n",
    "    # Add year, month, day as coordinates\n",
    "    ds = ds.assign_coords(\n",
    "        year=ds['valid_time'].dt.year,\n",
    "        month=ds['valid_time'].dt.month,\n",
    "        day=ds['valid_time'].dt.day\n",
    "    )\n",
    "    \n",
    "    # Temporary list for this year\n",
    "    df_list = []\n",
    "    \n",
    "    # Loop over each unique month in this file\n",
    "    for month_val in np.unique(ds['month'].values):\n",
    "        days_in_month = np.unique(ds['day'].values[ds['month'].values == month_val])\n",
    "        \n",
    "        # Loop over each day in the month with a progress bar\n",
    "        for day_val in tqdm(days_in_month, desc=f\"Month {month_val}\", leave=False):\n",
    "            ds_day = ds.sel(\n",
    "                valid_time=(ds['month'] == month_val) & (ds['day'] == day_val)\n",
    "            )\n",
    "            if ds_day['valid_time'].size == 0:\n",
    "                continue\n",
    "            \n",
    "            wind_data = ds_day['wind_speed'].values\n",
    "\n",
    "            # Compute daily statistics\n",
    "            mean = np.nanmean(wind_data, axis=0)\n",
    "            median = np.nanmedian(wind_data, axis=0)\n",
    "            std = np.nanstd(wind_data, axis=0)\n",
    "            max_ = np.nanmax(wind_data, axis=0)\n",
    "            min_ = np.nanmin(wind_data, axis=0)\n",
    "\n",
    "            # Create Dataset for this day's stats\n",
    "            stats = xr.Dataset({\n",
    "                'Mean': (['latitude', 'longitude'], mean),\n",
    "                'Median': (['latitude', 'longitude'], median),\n",
    "                'Std': (['latitude', 'longitude'], std),\n",
    "                'Max': (['latitude', 'longitude'], max_),\n",
    "                'Min': (['latitude', 'longitude'], min_)\n",
    "            }, coords={'latitude': ds['latitude'], 'longitude': ds['longitude']})\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            stats_df = stats.to_dataframe().reset_index()\n",
    "\n",
    "            # Add time labels\n",
    "            stats_df['Year'] = year_val\n",
    "            stats_df['Month'] = month_val\n",
    "            stats_df['Day'] = int(day_val)\n",
    "\n",
    "            # Set multi-index\n",
    "            stats_df = stats_df.set_index(['Year', 'Month', 'Day', 'latitude', 'longitude'])\n",
    "            df_list.append(stats_df)\n",
    "    \n",
    "    # Save this year's results\n",
    "    if df_list:  # Only save if data exists\n",
    "        df_final = pd.concat(df_list)\n",
    "        df_final_xr = df_final.reset_index().set_index(['Year', 'Month', 'Day', 'latitude', 'longitude'])\n",
    "        df_final_xr = df_final_xr.to_xarray()\n",
    "        \n",
    "        output_file_path = os.path.join(output_folder, f\"daily_wind_speed_stats_{year_val}.nc\")\n",
    "        df_final_xr.to_netcdf(output_file_path)\n",
    "        print(f\"âœ… Saved {output_file_path}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All yearly files saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c693b14",
   "metadata": {},
   "source": [
    "Convert lat and lon from 0.25 to 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input/output folders\n",
    "input_folder = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly\"\n",
    "output_folder = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of input NetCDF files\n",
    "file_list = sorted([f for f in os.listdir(input_folder) if f.endswith(\".nc\")])\n",
    "print(f\"Found {len(file_list)} yearly files to regrid...\")\n",
    "\n",
    "# Regrid parameters\n",
    "factor_lat = 3\n",
    "factor_lon = 3\n",
    "\n",
    "# Target grid\n",
    "lat_target = np.arange(34.5, 66.0 + 0.001, 0.75)   # 43 lats\n",
    "lon_target = np.arange(-12.0, 36.0 + 0.001, 0.75)  # 65 lons\n",
    "\n",
    "# Loop through files\n",
    "for file in tqdm(file_list, desc=\"Regridding yearly files\"):\n",
    "    file_path = os.path.join(input_folder, file)\n",
    "    year_val = file.split('_')[-1][:4]  # extract year from filename\n",
    "    \n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Step 1: Coarsen (aggregation)\n",
    "    ds_coarse = xr.Dataset()\n",
    "    for var in [\"Mean\", \"Median\", \"Std\", \"Max\", \"Min\"]:\n",
    "        if var in ds:\n",
    "            ds_coarse[var] = ds[var].coarsen(\n",
    "                latitude=factor_lat, longitude=factor_lon, boundary=\"trim\"\n",
    "            ).mean(skipna=True)\n",
    "\n",
    "    # Handle \"number\" only if it's in the dataset AND has lat/lon dims\n",
    "    if \"number\" in ds and {\"latitude\", \"longitude\"}.issubset(ds[\"number\"].dims):\n",
    "        ds_coarse[\"number\"] = ds[\"number\"].coarsen(\n",
    "            latitude=factor_lat, longitude=factor_lon, boundary=\"trim\"\n",
    "        ).sum(skipna=True)\n",
    "\n",
    "    # Copy non-spatial coords (if they align)\n",
    "    for coord in [\"Year\", \"Month\", \"Day\"]:\n",
    "        if coord in ds:\n",
    "            ds_coarse[coord] = ds[coord]\n",
    "    \n",
    "    # Step 2: Interpolate onto target grid\n",
    "    ds_final = ds_coarse.interp(\n",
    "        latitude=lat_target,\n",
    "        longitude=lon_target,\n",
    "        method=\"linear\",\n",
    "        kwargs={\"fill_value\": \"extrapolate\"}\n",
    "    )\n",
    "    \n",
    "    # Save output\n",
    "    out_file = os.path.join(output_folder, f\"daily_wind_speed_stats_{year_val}_regrid.nc\")\n",
    "    ds_final.to_netcdf(out_file)\n",
    "    print(f\"âœ… Saved {out_file}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All yearly wind speed files regridded and saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
