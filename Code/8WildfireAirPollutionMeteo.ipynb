{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BI Goals:\n",
    "\n",
    "i) investigar as ligações entre a atividade do fogo, conforme medido pelo FRP (Fire Radiative Power), e as concentrações de poluentes e avaliar a zona espacial e temporal de influência da atividade dos incêndios florestais.\n",
    "\n",
    "(ii) investigar a utilização de FRP como ferramenta para filtrar a contribuição do fumo de biomassa para os registos de poluição atmosférica em bacias atmosféricas urbanas, nomeadamente as emissões de carbono resultantes de incêndios florestais graves.\n",
    "\n",
    "(iii) desenvolver abordagens multirriscos para caracterizar o comportamento conjunto de múltiplos perigos e riscos consequentes e avaliar o papel desempenhado por condições anteriores e simultâneas de seca e/ou calor na exacerbação de incêndios rurais e consequentes ondas de fumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine datasets (labeled dataset that contains info about labels and FRP & pollutants statistics) by day and pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading pollutant, fire, and mask datasets...\n",
      "🌦 Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\3414873921.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\3414873921.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\3414873921.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\3414873921.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Common time range: 2003-01-01 → 2024-12-25\n",
      "   Total valid days: 8030\n",
      "📏 Checking spatial consistency...\n",
      "✅ All datasets share identical lat/lon grids.\n",
      "🎭 Applying land/region mask...\n",
      "🔗 Merging all datasets...\n",
      "\n",
      "🎉 Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\pm2p5_fire_meteo_Greece.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm2p5\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Greece\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"📂 Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"🌦 Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"✅ Common time range: {pd.to_datetime(start_time).date()} → {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"📏 Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"✅ All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"🎭 Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"🔗 Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading pollutant, fire, and mask datasets...\n",
      "🌦 Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2700361340.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2700361340.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2700361340.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2700361340.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Common time range: 2003-01-01 → 2024-12-25\n",
      "   Total valid days: 8030\n",
      "📏 Checking spatial consistency...\n",
      "✅ All datasets share identical lat/lon grids.\n",
      "🎭 Applying land/region mask...\n",
      "🔗 Merging all datasets...\n",
      "\n",
      "🎉 Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\pm10_fire_meteo_Greece.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm10\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Greece\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"📂 Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"🌦 Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"✅ Common time range: {pd.to_datetime(start_time).date()} → {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"📏 Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"✅ All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"🎭 Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"🔗 Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading pollutant, fire, and mask datasets...\n",
      "🌦 Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\953619958.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\953619958.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\953619958.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\953619958.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Common time range: 2003-01-01 → 2024-12-25\n",
      "   Total valid days: 8030\n",
      "📏 Checking spatial consistency...\n",
      "✅ All datasets share identical lat/lon grids.\n",
      "🎭 Applying land/region mask...\n",
      "🔗 Merging all datasets...\n",
      "\n",
      "🎉 Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\co_fire_meteo_Greece.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"co\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Greece\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"📂 Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"🌦 Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"✅ Common time range: {pd.to_datetime(start_time).date()} → {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"📏 Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"✅ All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"🎭 Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"🔗 Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading pollutant, fire, and mask datasets...\n",
      "🌦 Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2949953496.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2949953496.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2949953496.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2949953496.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Common time range: 2003-01-01 → 2024-12-25\n",
      "   Total valid days: 8030\n",
      "📏 Checking spatial consistency...\n",
      "✅ All datasets share identical lat/lon grids.\n",
      "🎭 Applying land/region mask...\n",
      "🔗 Merging all datasets...\n",
      "\n",
      "🎉 Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\no_fire_meteo_Greece.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Greece\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"📂 Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"🌦 Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"✅ Common time range: {pd.to_datetime(start_time).date()} → {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"📏 Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"✅ All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"🎭 Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"🔗 Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading pollutant, fire, and mask datasets...\n",
      "🌦 Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2952296630.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2952296630.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2952296630.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_3356\\2952296630.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Common time range: 2003-01-01 → 2024-12-25\n",
      "   Total valid days: 8030\n",
      "📏 Checking spatial consistency...\n",
      "✅ All datasets share identical lat/lon grids.\n",
      "🎭 Applying land/region mask...\n",
      "🔗 Merging all datasets...\n",
      "\n",
      "🎉 Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\no2_fire_meteo_Greece.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no2\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Greece\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"📂 Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"🌦 Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"✅ Common time range: {pd.to_datetime(start_time).date()} → {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"📏 Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"✅ All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"🎭 Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"🔗 Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not In Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid PM2P5 days: 8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15836\\3172340906.py:59: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overlapping valid days (PM2P5 + fire): 8030\n",
      "   Overlap span: 2003-01-01 → 2024-12-25\n",
      "\n",
      "🎉 Merged dataset saved to: D:\\IPMA\\CAMS\\pm2p5_fire_Iberia.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm2p5\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"✅ Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"✅ Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} → {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid PM10 days: 8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15836\\3129533668.py:59: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overlapping valid days (PM10 + fire): 8030\n",
      "   Overlap span: 2003-01-01 → 2024-12-25\n",
      "\n",
      "🎉 Merged dataset saved to: D:\\IPMA\\CAMS\\pm10_fire_Iberia.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm10\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"✅ Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"✅ Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} → {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid NO2 days: 8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15836\\1260313734.py:59: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overlapping valid days (NO2 + fire): 8030\n",
      "   Overlap span: 2003-01-01 → 2024-12-25\n",
      "\n",
      "🎉 Merged dataset saved to: D:\\IPMA\\CAMS\\no2_fire_Iberia.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no2\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"✅ Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"✅ Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} → {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid NO days: 8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15836\\1744303364.py:59: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overlapping valid days (NO + fire): 8030\n",
      "   Overlap span: 2003-01-01 → 2024-12-25\n",
      "\n",
      "🎉 Merged dataset saved to: D:\\IPMA\\CAMS\\no_fire_Iberia.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"✅ Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"✅ Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} → {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total valid CO days: 8036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15836\\1972906279.py:59: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overlapping valid days (CO + fire): 8030\n",
      "   Overlap span: 2003-01-01 → 2024-12-25\n",
      "\n",
      "🎉 Merged dataset saved to: D:\\IPMA\\CAMS\\co_fire_Iberia.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"co\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"✅ Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"✅ Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} → {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\n🎉 Merged dataset saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
