{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BI Goals:\n",
    "\n",
    "i) investigar as ligaÃ§Ãµes entre a atividade do fogo, conforme medido pelo FRP (Fire Radiative Power), e as concentraÃ§Ãµes de poluentes e avaliar a zona espacial e temporal de influÃªncia da atividade dos incÃªndios florestais.\n",
    "\n",
    "(ii) investigar a utilizaÃ§Ã£o de FRP como ferramenta para filtrar a contribuiÃ§Ã£o do fumo de biomassa para os registos de poluiÃ§Ã£o atmosfÃ©rica em bacias atmosfÃ©ricas urbanas, nomeadamente as emissÃµes de carbono resultantes de incÃªndios florestais graves.\n",
    "\n",
    "(iii) desenvolver abordagens multirriscos para caracterizar o comportamento conjunto de mÃºltiplos perigos e riscos consequentes e avaliar o papel desempenhado por condiÃ§Ãµes anteriores e simultÃ¢neas de seca e/ou calor na exacerbaÃ§Ã£o de incÃªndios rurais e consequentes ondas de fumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine datasets (labeled dataset that contains info about labels and FRP & pollutants statistics) by day and pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading pollutant, fire, and mask datasets...\n",
      "ğŸŒ¦ Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3410244454.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3410244454.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3410244454.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3410244454.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Common time range: 2003-01-01 â†’ 2024-12-25\n",
      "   Total valid days: 8030\n",
      "ğŸ“ Checking spatial consistency...\n",
      "âœ… All datasets share identical lat/lon grids.\n",
      "ğŸ­ Applying land/region mask...\n",
      "ğŸ”— Merging all datasets...\n",
      "\n",
      "ğŸ‰ Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\pm2p5_fire_meteo_Portugal.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm2p5\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Portugal\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"ğŸ“‚ Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"ğŸŒ¦ Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"âœ… Common time range: {pd.to_datetime(start_time).date()} â†’ {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"ğŸ“ Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"âœ… All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"ğŸ­ Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"ğŸ”— Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading pollutant, fire, and mask datasets...\n",
      "ğŸŒ¦ Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3415694762.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3415694762.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3415694762.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\3415694762.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Common time range: 2003-01-01 â†’ 2024-12-25\n",
      "   Total valid days: 8030\n",
      "ğŸ“ Checking spatial consistency...\n",
      "âœ… All datasets share identical lat/lon grids.\n",
      "ğŸ­ Applying land/region mask...\n",
      "ğŸ”— Merging all datasets...\n",
      "\n",
      "ğŸ‰ Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\pm10_fire_meteo_Portugal.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm10\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Portugal\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"ğŸ“‚ Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"ğŸŒ¦ Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"âœ… Common time range: {pd.to_datetime(start_time).date()} â†’ {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"ğŸ“ Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"âœ… All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"ğŸ­ Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"ğŸ”— Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading pollutant, fire, and mask datasets...\n",
      "ğŸŒ¦ Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\2946299961.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\2946299961.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\2946299961.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\2946299961.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Common time range: 2003-01-01 â†’ 2024-12-25\n",
      "   Total valid days: 8030\n",
      "ğŸ“ Checking spatial consistency...\n",
      "âœ… All datasets share identical lat/lon grids.\n",
      "ğŸ­ Applying land/region mask...\n",
      "ğŸ”— Merging all datasets...\n",
      "\n",
      "ğŸ‰ Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\co_fire_meteo_Portugal.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"co\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Portugal\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"ğŸ“‚ Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"ğŸŒ¦ Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"âœ… Common time range: {pd.to_datetime(start_time).date()} â†’ {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"ğŸ“ Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"âœ… All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"ğŸ­ Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"ğŸ”— Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading pollutant, fire, and mask datasets...\n",
      "ğŸŒ¦ Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\164806666.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\164806666.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\164806666.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\164806666.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Common time range: 2003-01-01 â†’ 2024-12-25\n",
      "   Total valid days: 8030\n",
      "ğŸ“ Checking spatial consistency...\n",
      "âœ… All datasets share identical lat/lon grids.\n",
      "ğŸ­ Applying land/region mask...\n",
      "ğŸ”— Merging all datasets...\n",
      "\n",
      "ğŸ‰ Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\no_fire_meteo_Portugal.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Portugal\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"ğŸ“‚ Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"ğŸŒ¦ Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"âœ… Common time range: {pd.to_datetime(start_time).date()} â†’ {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"ğŸ“ Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"âœ… All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"ğŸ­ Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"ğŸ”— Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading pollutant, fire, and mask datasets...\n",
      "ğŸŒ¦ Loading meteorological datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\777608923.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\777608923.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\777608923.py:70: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_stack = ds_stack.assign_coords(time=time_index)\n",
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15680\\777608923.py:100: FutureWarning: updating coordinate 'time' with a PandasMultiIndex would leave the multi-index level coordinates ['Year', 'Month', 'Day'] in an inconsistent state. This will raise an error in the future. Use `.drop_vars(['time', 'Year', 'Month', 'Day'])` before assigning new coordinate values.\n",
      "  ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Common time range: 2003-01-01 â†’ 2024-12-25\n",
      "   Total valid days: 8030\n",
      "ğŸ“ Checking spatial consistency...\n",
      "âœ… All datasets share identical lat/lon grids.\n",
      "ğŸ­ Applying land/region mask...\n",
      "ğŸ”— Merging all datasets...\n",
      "\n",
      "ğŸ‰ Merged dataset successfully saved to:\n",
      "   D:\\IPMA\\Results\\no2_fire_meteo_Portugal.nc\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no2\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Portugal\"        # <-- change this to your region/country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_meteo_{country}.nc\"\n",
    "\n",
    "# Meteorological folders\n",
    "precip_path = r\"D:\\IPMA\\ERA5\\Precipitation\\daily_precipitation_stats_yearly_regridded\"\n",
    "temp_path = r\"D:\\IPMA\\ERA5\\Temperature\\daily_temperature_stats_yearly_regridded\"\n",
    "wind_path = r\"D:\\IPMA\\ERA5\\UV_wind\\daily_wind_speed_stats_yearly_regridded\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "print(\"ğŸ“‚ Loading pollutant, fire, and mask datasets...\")\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "mask = mask_ds[\"mask\"]  # adjust if mask variable has a different name\n",
    "\n",
    "# ==== Helper function to load yearly meteorological data ====\n",
    "def load_meteorological_dataset(folder_path, prefix, variables_to_keep):\n",
    "    \"\"\"Load all yearly ERA5 NetCDF files, keep only selected variables, and flatten into a time dimension.\"\"\"\n",
    "    files = sorted([os.path.join(folder_path, f)\n",
    "                    for f in os.listdir(folder_path)\n",
    "                    if f.endswith(\".nc\")])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No NetCDF files found in {folder_path}\")\n",
    "    \n",
    "    ds_list = []\n",
    "    for f in files:\n",
    "        ds = xr.open_dataset(f)\n",
    "        # Keep only desired variables\n",
    "        keep_vars = [v for v in ds.data_vars if v in variables_to_keep]\n",
    "        ds = ds[keep_vars]\n",
    "        # Rename to avoid variable name collisions (add prefix)\n",
    "        rename_dict = {v: f\"{prefix}_{v}\" for v in keep_vars}\n",
    "        ds = ds.rename(rename_dict)\n",
    "        ds_list.append(ds)\n",
    "    \n",
    "    # Concatenate yearly datasets\n",
    "    ds_all = xr.concat(ds_list, dim=\"Year\")\n",
    "    \n",
    "    # Build valid date list\n",
    "    years = ds_all['Year'].values\n",
    "    months = ds_all['Month'].values\n",
    "    days = ds_all['Day'].values\n",
    "    ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                     names=['year', 'month', 'day']).to_frame(index=False)\n",
    "    def is_valid_date(row):\n",
    "        try:\n",
    "            pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "    ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "    time_index = pd.to_datetime(ymd_valid)\n",
    "    \n",
    "    # Stack and assign new time coordinate\n",
    "    ds_stack = ds_all.stack(time=('Year', 'Month', 'Day'))\n",
    "    ds_stack = ds_stack.isel(time=valid_mask.values)\n",
    "    ds_stack = ds_stack.assign_coords(time=time_index)\n",
    "    return ds_stack\n",
    "\n",
    "# ==== Load meteorological datasets ====\n",
    "print(\"ğŸŒ¦ Loading meteorological datasets...\")\n",
    "ds_precip = load_meteorological_dataset(precip_path, \"precip\", [\"Total_Precipitation\", \"Max\", \"Mean\"])\n",
    "ds_temp = load_meteorological_dataset(temp_path, \"temp\", [\"Mean\", \"Max\"])\n",
    "ds_wind = load_meteorological_dataset(wind_path, \"wind\", [\"Mean\", \"Max\"])\n",
    "\n",
    "# ==== Flatten pollutant dataset ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "ymd = pd.MultiIndex.from_product([years, months, days],\n",
    "                                 names=['year', 'month', 'day']).to_frame(index=False)\n",
    "\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Align time ranges ====\n",
    "start_time = max(\n",
    "    ds_pollutant_stack.time.values[0],\n",
    "    ds_fire.time.values[0],\n",
    "    ds_precip.time.values[0],\n",
    "    ds_temp.time.values[0],\n",
    "    ds_wind.time.values[0]\n",
    ")\n",
    "end_time = min(\n",
    "    ds_pollutant_stack.time.values[-1],\n",
    "    ds_fire.time.values[-1],\n",
    "    ds_precip.time.values[-1],\n",
    "    ds_temp.time.values[-1],\n",
    "    ds_wind.time.values[-1]\n",
    ")\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "ds_precip_sel = ds_precip.sel(time=slice(start_time, end_time))\n",
    "ds_temp_sel = ds_temp.sel(time=slice(start_time, end_time))\n",
    "ds_wind_sel = ds_wind.sel(time=slice(start_time, end_time))\n",
    "\n",
    "print(f\"âœ… Common time range: {pd.to_datetime(start_time).date()} â†’ {pd.to_datetime(end_time).date()}\")\n",
    "print(f\"   Total valid days: {len(ds_pollutant_sel.time)}\")\n",
    "\n",
    "# ==== Spatial alignment checks ====\n",
    "print(\"ğŸ“ Checking spatial consistency...\")\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch (fire)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch (mask)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_precip_sel.latitude.values), \"Latitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_precip_sel.longitude.values), \"Longitude mismatch (precip)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_temp_sel.latitude.values), \"Latitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_temp_sel.longitude.values), \"Longitude mismatch (temp)\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_wind_sel.latitude.values), \"Latitude mismatch (wind)\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_wind_sel.longitude.values), \"Longitude mismatch (wind)\"\n",
    "print(\"âœ… All datasets share identical lat/lon grids.\")\n",
    "\n",
    "# ==== Apply mask ====\n",
    "print(\"ğŸ­ Applying land/region mask...\")\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "ds_precip_masked = ds_precip_sel.where(mask == 1)\n",
    "ds_temp_masked = ds_temp_sel.where(mask == 1)\n",
    "ds_wind_masked = ds_wind_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge everything ====\n",
    "print(\"ğŸ”— Merging all datasets...\")\n",
    "ds_merged = xr.merge([\n",
    "    ds_pollutant_masked,\n",
    "    ds_fire_masked,\n",
    "    ds_precip_masked,\n",
    "    ds_temp_masked,\n",
    "    ds_wind_masked\n",
    "])\n",
    "\n",
    "# ==== Save final dataset ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset successfully saved to:\\n   {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not In Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm2p5\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"âœ… Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"âœ… Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} â†’ {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"pm10\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_singlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"âœ… Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"âœ… Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} â†’ {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no2\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"âœ… Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"âœ… Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} â†’ {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"no\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"âœ… Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"âœ… Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} â†’ {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== User settings ====\n",
    "pollutant_name = \"co\"  # <-- change this to your pollutant (e.g., \"ozone\", \"no2\")\n",
    "country = \"Iberia\"         # <-- change this to the country\n",
    "\n",
    "# ==== File paths ====\n",
    "pollutant_path = fr\"D:\\IPMA\\CAMS\\chem_multlvl\\daily_{pollutant_name}_stats.nc\"\n",
    "fire_path = fr\"D:\\IPMA\\FRP\\fire_labels_by_region\\fire_data_{country}.nc\"\n",
    "mask_path = fr\"D:\\IPMA\\Countries\\{country}_mask.nc\"\n",
    "output_path = fr\"D:\\IPMA\\Results\\{pollutant_name}_fire_{country}.nc\"\n",
    "\n",
    "# ==== Load datasets ====\n",
    "ds_pollutant = xr.open_dataset(pollutant_path)\n",
    "ds_fire = xr.open_dataset(fire_path)\n",
    "mask_ds = xr.open_dataset(mask_path)\n",
    "\n",
    "# ==== Load mask variable ====\n",
    "mask = mask_ds[\"mask\"]  # change name here if different\n",
    "\n",
    "# ==== Extract Year, Month, Day ====\n",
    "years = ds_pollutant['Year'].values\n",
    "months = ds_pollutant['Month'].values\n",
    "days = ds_pollutant['Day'].values\n",
    "\n",
    "# ==== Create all combinations ====\n",
    "ymd = pd.MultiIndex.from_product(\n",
    "    [years, months, days],\n",
    "    names=['year', 'month', 'day']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# ==== Remove invalid dates ====\n",
    "def is_valid_date(row):\n",
    "    try:\n",
    "        pd.Timestamp(year=int(row['year']), month=int(row['month']), day=int(row['day']))\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "valid_mask = ymd.apply(is_valid_date, axis=1)\n",
    "ymd_valid = ymd[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# ==== Convert valid dates to datetime ====\n",
    "time_index = pd.to_datetime(ymd_valid)\n",
    "\n",
    "# ==== Print number of valid pollutant days ====\n",
    "print(f\"âœ… Total valid {pollutant_name.upper()} days: {len(time_index)}\")\n",
    "\n",
    "# ==== Flatten pollutant data ====\n",
    "ds_pollutant_stack = ds_pollutant.stack(time=('Year', 'Month', 'Day'))\n",
    "\n",
    "# Keep only valid dates\n",
    "ds_pollutant_stack = ds_pollutant_stack.isel(time=valid_mask.values)\n",
    "\n",
    "# Assign new datetime coordinate\n",
    "ds_pollutant_stack = ds_pollutant_stack.assign_coords(time=time_index)\n",
    "\n",
    "# ==== Select overlapping time ====\n",
    "start_time = max(ds_pollutant_stack.time.values[0], ds_fire.time.values[0])\n",
    "end_time = min(ds_pollutant_stack.time.values[-1], ds_fire.time.values[-1])\n",
    "\n",
    "ds_pollutant_sel = ds_pollutant_stack.sel(time=slice(start_time, end_time))\n",
    "ds_fire_sel = ds_fire.sel(time=slice(start_time, end_time))\n",
    "\n",
    "# ==== Print number of overlapping valid days ====\n",
    "print(f\"âœ… Overlapping valid days ({pollutant_name.upper()} + fire): {len(ds_pollutant_sel.time)}\")\n",
    "print(f\"   Overlap span: {pd.to_datetime(ds_pollutant_sel.time.values[0]).date()} â†’ {pd.to_datetime(ds_pollutant_sel.time.values[-1]).date()}\")\n",
    "\n",
    "# ==== Check lat/lon match ====\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, ds_fire_sel.latitude.values), \"Latitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, ds_fire_sel.longitude.values), \"Longitude mismatch\"\n",
    "assert np.allclose(ds_pollutant_sel.latitude.values, mask.latitude.values), \"Latitude mismatch with mask\"\n",
    "assert np.allclose(ds_pollutant_sel.longitude.values, mask.longitude.values), \"Longitude mismatch with mask\"\n",
    "\n",
    "# ==== Apply mask to both datasets ====\n",
    "ds_pollutant_masked = ds_pollutant_sel.where(mask == 1)\n",
    "ds_fire_masked = ds_fire_sel.where(mask == 1)\n",
    "\n",
    "# ==== Merge datasets ====\n",
    "ds_merged = xr.merge([ds_pollutant_masked, ds_fire_masked])\n",
    "\n",
    "# ==== Save ====\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "ds_merged.to_netcdf(output_path)\n",
    "\n",
    "print(f\"\\nğŸ‰ Merged dataset saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
