{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pollutants PM10 & PM2.5, NO2, CO2, O3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMS global reanalysis EAC4 - Data Preparation (according to what Virgilio gave in matlab scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\CAMS\\chem_singlvl\\1raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_singlvl_2024.nc\" #change path accordingly\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Iterate through years and months\n",
    "dest_folder = r\"E:\\IPMA\\CAMS\\chem_singlvl\\1raw_2023_2024\"\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        # Filter dataset for the given year and month\n",
    "        monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "        if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "            output_filename = rf\"{dest_folder}\\CAMS_global_reanalysis_EAC4_chem_singlvl_{year}{month:02d}.nc\"\n",
    "            monthly_ds.to_netcdf(output_filename)\n",
    "            print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change valid_time to time (so it matches) - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to your directory containing the NetCDF files\n",
    "input_dir = r\"E:\\IPMA\\CAMS\\chem_singlvl\\1raw_2023_2024\"\n",
    "output_dir = r\"E:\\IPMA\\CAMS\\chem_singlvl\\4final_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Loop through each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Check if 'valid_time' exists and rename it to 'time'\n",
    "    if 'valid_time' in ds:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "    # Get the filename and create the output path in the new directory\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the updated dataset to the output directory (overwrite original file in the new folder)\n",
    "    ds.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Renamed 'valid_time' to 'time' and saved to {output_file}\")\n",
    "\n",
    "print(\"Renaming and saving to new folder complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert coords from 0 to 360 to -180 to 180 - DONE for 2003-2022\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r\"D:\\CAMS\\CAMS_global_reanalysis_EAC4\\chem_singlvl\\raw_2003_2022\"\n",
    "output_folder = r\"D:\\CAMS\\CAMS_global_reanalysis_EAC4\\chem_singlvl\\transformation_2003_2022\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of NetCDF files in the input folder\n",
    "nc_files = [f for f in os.listdir(input_folder) if f.endswith(\".nc\")]\n",
    "\n",
    "# Process each file\n",
    "for nc_file in nc_files:\n",
    "    input_path = os.path.join(input_folder, nc_file)\n",
    "    output_path = os.path.join(output_folder, nc_file)\n",
    "\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Extract longitude and latitude\n",
    "    lon = ds['longitude'].values  # (480,)\n",
    "    lat = ds['latitude'].values   # (241,)\n",
    "\n",
    "    # Convert longitude from 0-360 to -180 to 180\n",
    "    lon2 = (lon + 180) % 360 - 180\n",
    "\n",
    "    # Swap the first and second halves\n",
    "    lon3 = np.copy(lon2)\n",
    "    lon3[:240] = lon2[240:480]\n",
    "    lon3[240:480] = lon2[:240]\n",
    "\n",
    "    # Create a meshgrid (not strictly needed for saving, but useful)\n",
    "    LON, LAT = np.meshgrid(lon3, lat)\n",
    "\n",
    "    # Apply the same transformation to pm10, pm1, and pm2p5\n",
    "    for var in [\"pm10\", \"pm1\", \"pm2p5\"]:\n",
    "        if var in ds:\n",
    "            data = ds[var].values  # Shape: (time, lat, lon)\n",
    "            transformed_data = np.copy(data)\n",
    "\n",
    "            # Swap the longitude axis (last axis)\n",
    "            transformed_data[:, :, :240] = data[:, :, 240:480]\n",
    "            transformed_data[:, :, 240:480] = data[:, :, :240]\n",
    "\n",
    "            # Replace the dataset variable with the corrected data\n",
    "            ds[var].values = transformed_data\n",
    "\n",
    "    # Update longitude in the dataset\n",
    "    ds = ds.assign_coords(longitude=lon3)\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Saved transformed file: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - DONE for 2003-2022\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define your study area (lat_max, lon_min, lat_min, lon_max)\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = r\"D:\\CAMS\\CAMS_global_reanalysis_EAC4\\chem_singlvl\\transformation_2003_2022\"\n",
    "output_dir = r\"D:\\CAMS\\CAMS_global_reanalysis_EAC4\\chem_singlvl\\cropped_2003_2022\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Ensure latitude slicing is correct (since it decreases from 90 to -90)\n",
    "    ds_cropped = ds.sel(\n",
    "        latitude=slice(lat_max, lat_min),  # lat_max is greater than lat_min\n",
    "        longitude=slice(lon_min, lon_max)  # lon_min is less than lon_max\n",
    "    )\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the cropped dataset\n",
    "    ds_cropped.to_netcdf(output_file)\n",
    "    print(f\"Saved cropped file to {output_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put together files for each year - DONE for 2003-2024 (final step)\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the original path where the files are located\n",
    "input_path = r\"E:\\IPMA\\CAMS\\chem_singlvl\\5conversion_2003_2024\"\n",
    "\n",
    "# Define the new path where you want to save the compiled files\n",
    "output_path_base = r\"E:\\IPMA\\CAMS\\chem_singlvl\\6compile_2003_2024\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_path_base, exist_ok=True)\n",
    "\n",
    "# Loop over each year from 2003 to 2024\n",
    "for year in range(2003, 2025):\n",
    "    # Create the file pattern for the specific year (e.g., 2003*)\n",
    "    file_pattern = os.path.join(input_path, f\"CAMS_global_reanalysis_EAC4_chem_singlvl_{year}*.nc\")\n",
    "    \n",
    "    # Use glob to find all .nc files for the specified year\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if files:  # Only proceed if there are files for that year\n",
    "        # Open all files for the year and concatenate along the 'time' dimension\n",
    "        ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "\n",
    "        # Save the compiled dataset to the new path\n",
    "        output_path = os.path.join(output_path_base, f\"CAMS_global_reanalysis_EAC4_chem_singlvl_{year}.nc\")\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Files for {year} have been compiled into: {output_path}\")\n",
    "    else:\n",
    "        print(f\"No files found for {year}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERA5 - all 5 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To separate year by year files downloaded with more than 1 year each - DONE for 1979-2024\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\ERA5\\UV_wind\\ERA5_hourly_uv_2003_1999.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Destination folder to save yearly files\n",
    "dest_folder = r\"E:\\IPMA\\ERA5\\UV_wind\"\n",
    "\n",
    "# Iterate through years\n",
    "for year in years:\n",
    "    # Filter dataset for the given year\n",
    "    yearly_ds = ds.sel(valid_time=ds.valid_time.dt.year == year)\n",
    "\n",
    "    if yearly_ds.valid_time.size > 0:  # Only save if data exists for the year\n",
    "        output_filename = rf\"{dest_folder}\\ERA5_hourly_uv_{year}.nc\" #change accordingly to what file is being used\n",
    "        yearly_ds.to_netcdf(output_filename)\n",
    "        print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 1979-2024\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Directory where all .nc files are located\n",
    "file_path = r\"E:\\IPMA\\ERA5\\UV_wind\\1raw_year_1979_2024\"\n",
    "\n",
    "# Destination folder for monthly files\n",
    "dest_folder = r\"E:\\IPMA\\ERA5\\UV_wind\\1raw_month_1979_2024\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(file_path):\n",
    "    if filename.endswith(\".nc\"):\n",
    "        file_full_path = os.path.join(file_path, filename)\n",
    "        \n",
    "        # Load the dataset\n",
    "        ds = xr.open_dataset(file_full_path)\n",
    "\n",
    "        # Ensure valid_time is a datetime object\n",
    "        ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "        # Get unique years in the dataset\n",
    "        years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "        # Iterate through years and months\n",
    "        for year in years:\n",
    "            for month in range(1, 13):\n",
    "                # Filter dataset for the given year and month\n",
    "                monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "                if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "                    output_filename = rf\"{dest_folder}\\ERA5_hourly_uv_{year}{month:02d}.nc\" #change accordingly to what file is being used\n",
    "                    monthly_ds.to_netcdf(output_filename)\n",
    "                    print(f\"Saved {output_filename}\")\n",
    "\n",
    "        # Close the dataset\n",
    "        ds.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put together all the csv files into one - DONE\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = r\"E:\\IPMA\\FRP\"\n",
    "\n",
    "# Get all CSV file paths from the folder\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# Read and concatenate the files\n",
    "df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset\n",
    "df.to_csv(os.path.join(folder_path, \"FRP_2001_2023.csv\"), index=False)\n",
    "\n",
    "print(\"All CSV files in the folder merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert .npy to .nc files - RAQUEL SOURCE - DONE\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing .npy files\n",
    "input_folder = r\"E:\\IPMA\\SPI\\SPI3\"\n",
    "output_folder = r\"E:\\IPMA\\SPI\\SPI3\\nc\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all .npy files in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".npy\"):  # Process only .npy files\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        data = np.load(file_path)  # Load the 3D array\n",
    "\n",
    "        # Define dimensions (adjust accordingly)\n",
    "        dims = (\"time\", \"lat\", \"lon\")  # Change based on data structure\n",
    "        coords = {\n",
    "            \"time\": np.arange(data.shape[0]),  # Modify based on actual data\n",
    "            \"lat\": np.linspace(34, 66, data.shape[1]),  # Modify latitudes\n",
    "            \"lon\": np.linspace(-12, 36, data.shape[2])  # Modify longitudes\n",
    "        }\n",
    "\n",
    "        # Convert to xarray DataArray\n",
    "        da = xr.DataArray(data, dims=dims, coords=coords, name=\"spi03\")\n",
    "\n",
    "        # Convert to Dataset\n",
    "        ds = da.to_dataset(name=\"spi03\")\n",
    "\n",
    "        # Save as NetCDF file\n",
    "        output_filename = filename.replace(\".npy\", \".nc\")\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Converted {filename} -> {output_filename}\")\n",
    "\n",
    "print(\"Batch conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate spei for 1979-2024 - WEB SOURCE - DONE\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "def extract_spei_data(input_file, start_year=1979, end_year=2024):\n",
    "    \"\"\"\n",
    "    Extracts data from a NetCDF file for the years between start_year and end_year and saves it to a new NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input NetCDF file.\n",
    "        start_year (int): Start year for extraction (default 1979).\n",
    "        end_year (int): End year for extraction (default 2024).\n",
    "    \"\"\"\n",
    "    # Open the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    # Ensure time is in datetime format (if it's not already in datetime format)\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "\n",
    "    # Filter the data based on the time dimension (between the start and end year)\n",
    "    filtered_ds = ds.sel(time=slice(f\"{start_year}-01-01\", f\"{end_year}-12-31\"))\n",
    "    \n",
    "    # Create output filename dynamically based on the input file name and year range\n",
    "    base_filename = input_file.split('/')[-1].split('.')[0]  # Extract file name (e.g., spei_01)\n",
    "    output_file = f\"{base_filename}_{start_year}-{end_year}.nc\"\n",
    "    \n",
    "    # Save the filtered dataset to a new NetCDF file\n",
    "    filtered_ds.to_netcdf(output_file)\n",
    "    print(f\"Saved filtered data to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\spei12.nc\"\n",
    "extract_spei_data(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate spei for each year between 1979-2024 - WEB SOURCE - DONE\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "def extract_spei_data_by_year(input_file, start_year=1979, end_year=2024):\n",
    "    \"\"\"\n",
    "    Extracts data from a NetCDF file for each year between start_year and end_year\n",
    "    and saves each year as a separate NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input NetCDF file.\n",
    "        start_year (int): Start year for extraction (default 1979).\n",
    "        end_year (int): End year for extraction (default 2024).\n",
    "    \"\"\"\n",
    "    # Open the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    # Ensure time is in datetime format (if it's not already in datetime format)\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "\n",
    "    # Loop through each year and extract the data for that year\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Filter the dataset for the current year\n",
    "        filtered_ds = ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "        \n",
    "        # Create output filename for each year\n",
    "        base_filename = input_file.split('/')[-1].split('.')[0]  # Extract file name (e.g., spei_01)\n",
    "        output_file = f\"{base_filename}_{year}.nc\"\n",
    "        \n",
    "        # Save the filtered data for this year to a new NetCDF file\n",
    "        filtered_ds.to_netcdf(output_file)\n",
    "        print(f\"Saved filtered data for {year} to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\spei12.nc\"\n",
    "extract_spei_data_by_year(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - WEB SOURCE - DONE for 1979-2023\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# Define your input and output directories\n",
    "input_dir = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\1raw_1979_2023\"\n",
    "output_dir = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\2cropped_1979_2023\"\n",
    "\n",
    "# Define the latitude and longitude boundaries for your study area\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36  # Study area for Europe\n",
    "\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through all NC files in the input directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".nc\"):  # Check if it's a NetCDF file\n",
    "        input_file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "        # Open the NetCDF file using xarray\n",
    "        with xr.open_dataset(input_file_path) as ds:\n",
    "            # Crop the dataset to include only the specified region\n",
    "            ds_europe = ds.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))\n",
    "\n",
    "            # Create the output file path\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "            # Save the cropped data to a new NetCDF file\n",
    "            ds_europe.to_netcdf(output_file_path)\n",
    "            print(f\"Saved cropped file: {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
