{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34272ab",
   "metadata": {},
   "source": [
    "MAE - Mean Absolute Error (lower is better)\n",
    "\n",
    "RMSE - Root Mean Square Error (if RMSE >> MAE there is large outliers)\n",
    "\n",
    "Bias - bias > 0 (model systematically overpredicts); bias < 0 (model underpredicts); bias = 0 (unbiased)\n",
    "\n",
    "Correlation - corr = 1 (perfect temporal evolution); corr = 0 (no skill); corr < 0 (wrong dynamics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59985cc1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c813ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing: CO\n",
      "==============================\n",
      "Training samples: 1139999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 2/40\n",
      "\u001b[1m   22/35625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:03\u001b[0m 5ms/step - loss: 8.2919e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 3/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 4/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 5/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 6/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 7/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 11/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 12/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 13/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4ms/step - loss: 9.9870e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 9.9069e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 3ms/step - loss: 9.8388e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 4ms/step - loss: 9.7700e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 9.7104e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 9.6290e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 9.5826e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 9.5188e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 5ms/step - loss: 9.4631e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 5ms/step - loss: 9.4133e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 4ms/step - loss: 9.3485e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 9.3069e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 9.2478e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 9.2209e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 9.1605e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 5ms/step - loss: 9.1390e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 4ms/step - loss: 9.0950e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 9.0485e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 9.0014e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 8.9674e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 8.9292e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 8.8902e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 8.8710e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 8.8424e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 8.8096e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 8.7862e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 8.7616e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 4ms/step - loss: 8.7274e-04\n",
      "\n",
      "Evaluation (2017):\n",
      "  MAE:  0.0361\n",
      "  RMSE: 0.0361\n",
      "  Bias: 0.0038\n",
      "  Corr: 0.7106\n",
      "\n",
      "Saved results to: CO_LSTM_predictions_2017.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DAILY POLLUTANT PREDICTION WITH LSTM (SINGLE POLLUTANT)\n",
    "# NaNs are strictly ignored (no interpolation, no filling)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# FILE PATH (CHANGE THIS PER RUN)\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_NAME = \"CO\"   # for naming outputs only\n",
    "INPUT_FILE = r\"D:\\IPMA\\Results\\co_fire_meteo_Iberia.nc\"\n",
    "\n",
    "# ============================================================\n",
    "# USER SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_VAR = \"Mean\"\n",
    "TEST_YEAR = 2017\n",
    "TEST_MONTH = None        # None or 1–12\n",
    "LAG_DAYS = 14\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_VARS = [\n",
    "    \"Mean\",\n",
    "    \"temp_Max\",\n",
    "    \"wind_Max\",\n",
    "    \"precip_Total_Precipitation\",\n",
    "    \"frp_sum_Iberia\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def build_sequences(X, y, lags):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lags, len(X)):\n",
    "        X_out.append(X[i-lags:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "\n",
    "def train_mask(time):\n",
    "    return time.dt.year != TEST_YEAR\n",
    "\n",
    "\n",
    "def test_mask(time):\n",
    "    mask = time.dt.year == TEST_YEAR\n",
    "    if TEST_MONTH is not None:\n",
    "        mask = mask & (time.dt.month == TEST_MONTH)\n",
    "    return mask\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n==============================\")\n",
    "print(f\"Processing: {POLLUTANT_NAME}\")\n",
    "print(f\"==============================\")\n",
    "\n",
    "ds = xr.open_dataset(INPUT_FILE)\n",
    "\n",
    "# -------------------------------\n",
    "# CHECK VARIABLES\n",
    "# -------------------------------\n",
    "\n",
    "for var in INPUT_VARS:\n",
    "    if var not in ds:\n",
    "        raise KeyError(f\"{var} not found in {INPUT_FILE}\")\n",
    "\n",
    "y = ds[POLLUTANT_VAR]\n",
    "X = xr.merge([ds[var] for var in INPUT_VARS])\n",
    "\n",
    "time = ds.time\n",
    "train_idx = train_mask(time)\n",
    "test_idx = test_mask(time)\n",
    "\n",
    "X_train = X.sel(time=train_idx)\n",
    "y_train = y.sel(time=train_idx)\n",
    "X_test = X.sel(time=test_idx)\n",
    "y_test = y.sel(time=test_idx)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD TRAINING DATA (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for lat in ds.latitude.values:\n",
    "    for lon in ds.longitude.values:\n",
    "\n",
    "        X_ts = X_train.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y_train.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = (\n",
    "            ~np.isnan(y_ts) &\n",
    "            ~np.isnan(X_ts).any(axis=1)\n",
    "        )\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        if len(y_ts) <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_seq, y_seq = build_sequences(X_scaled, y_scaled, LAG_DAYS)\n",
    "\n",
    "        if len(y_seq) == 0:\n",
    "            continue\n",
    "\n",
    "        X_all.append(X_seq)\n",
    "        y_all.append(y_seq)\n",
    "\n",
    "X_train_all = np.concatenate(X_all)\n",
    "y_train_all = np.concatenate(y_all)\n",
    "\n",
    "print(f\"Training samples: {X_train_all.shape[0]}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL\n",
    "# ============================================================\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(LAG_DAYS, X_train_all.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_all,\n",
    "    y_train_all,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "test_times = time.sel(time=test_idx)\n",
    "\n",
    "preds = np.full(\n",
    "    (len(ds.latitude), len(ds.longitude), len(test_times)),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "for i, lat in enumerate(ds.latitude.values):\n",
    "    for j, lon in enumerate(ds.longitude.values):\n",
    "\n",
    "        X_ts = X.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = ~np.isnan(X_ts).any(axis=1)\n",
    "\n",
    "        if valid.sum() <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1))\n",
    "\n",
    "        test_start = np.where(test_idx.values)[0][0]\n",
    "\n",
    "        for k in range(len(test_times)):\n",
    "            t = test_start + k\n",
    "            if t - LAG_DAYS < 0:\n",
    "                continue\n",
    "\n",
    "            X_input = X_scaled[t-LAG_DAYS:t]\n",
    "\n",
    "            if np.isnan(X_input).any():\n",
    "                continue\n",
    "\n",
    "            pred_scaled = model.predict(\n",
    "                X_input.reshape(1, LAG_DAYS, -1),\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            preds[i, j, k] = scaler_y.inverse_transform(pred_scaled)[0, 0]\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "pred_da = xr.DataArray(\n",
    "    preds,\n",
    "    dims=(\"latitude\", \"longitude\", \"time\"),\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times\n",
    "    },\n",
    "    name=f\"{POLLUTANT_NAME}_predicted\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "error = pred_da - y_test\n",
    "\n",
    "mae = float(abs(error).where(np.isfinite(error)).mean())\n",
    "rmse = float(np.sqrt((error ** 2)).where(np.isfinite(error)).mean())\n",
    "bias = float(error.where(np.isfinite(error)).mean())\n",
    "corr = float(\n",
    "    xr.corr(pred_da, y_test, dim=\"time\")\n",
    "    .where(np.isfinite(pred_da))\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation (2017):\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  Bias: {bias:.4f}\")\n",
    "print(f\"  Corr: {corr:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS FOR MAPPING (NO PLOTTING)\n",
    "# ============================================================\n",
    "\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        f\"{POLLUTANT_NAME}_predicted\": pred_da,\n",
    "        f\"{POLLUTANT_NAME}_observed\": y_test,\n",
    "        f\"{POLLUTANT_NAME}_difference\": pred_da - y_test,\n",
    "    },\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add metadata (recommended)\n",
    "ds_out[f\"{POLLUTANT_NAME}_predicted\"].attrs[\"description\"] = \"LSTM predicted pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_observed\"].attrs[\"description\"] = \"Observed pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_difference\"].attrs[\"description\"] = \"Prediction minus observation\"\n",
    "\n",
    "ds_out.attrs[\"model\"] = \"LSTM\"\n",
    "ds_out.attrs[\"lags_days\"] = LAG_DAYS\n",
    "ds_out.attrs[\"test_year\"] = TEST_YEAR\n",
    "ds_out.attrs[\"pollutant\"] = POLLUTANT_NAME\n",
    "\n",
    "# Save to NetCDF\n",
    "output_file = f\"{POLLUTANT_NAME}_LSTM_predictions_{TEST_YEAR}.nc\"\n",
    "ds_out.to_netcdf(output_file)\n",
    "\n",
    "print(f\"\\nSaved results to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a324a2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing: NO\n",
      "==============================\n",
      "Training samples: 1139954\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0027\n",
      "Epoch 2/40\n",
      "\u001b[1m   29/35624\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 4ms/step - loss: 0.0029 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0026\n",
      "Epoch 3/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0025\n",
      "Epoch 4/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0025\n",
      "Epoch 5/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 5ms/step - loss: 0.0025\n",
      "Epoch 6/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 5ms/step - loss: 0.0025\n",
      "Epoch 7/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0024\n",
      "Epoch 8/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0024\n",
      "Epoch 9/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0024\n",
      "Epoch 10/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0024\n",
      "Epoch 11/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0024\n",
      "Epoch 12/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0023\n",
      "Epoch 13/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0023\n",
      "Epoch 14/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0023\n",
      "Epoch 15/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0023\n",
      "Epoch 16/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 17/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 18/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 19/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 20/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 4ms/step - loss: 0.0022\n",
      "Epoch 21/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0021\n",
      "Epoch 22/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 4ms/step - loss: 0.0021\n",
      "Epoch 23/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 5ms/step - loss: 0.0021\n",
      "Epoch 24/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 4ms/step - loss: 0.0021\n",
      "Epoch 25/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0021\n",
      "Epoch 26/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4ms/step - loss: 0.0021\n",
      "Epoch 27/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4ms/step - loss: 0.0020\n",
      "Epoch 28/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 5ms/step - loss: 0.0020\n",
      "Epoch 29/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 30/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 31/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 5ms/step - loss: 0.0020\n",
      "Epoch 32/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 6ms/step - loss: 0.0020\n",
      "Epoch 33/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 4ms/step - loss: 0.0020\n",
      "Epoch 34/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4ms/step - loss: 0.0020\n",
      "Epoch 35/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0020\n",
      "Epoch 36/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0019\n",
      "Epoch 37/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0019\n",
      "Epoch 38/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 4ms/step - loss: 0.0019\n",
      "Epoch 39/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0019\n",
      "Epoch 40/40\n",
      "\u001b[1m35624/35624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0019\n",
      "\n",
      "Evaluation (2017):\n",
      "  MAE:  1.1254\n",
      "  RMSE: 1.1254\n",
      "  Bias: 0.3871\n",
      "  Corr: 0.5132\n",
      "\n",
      "Saved results to: NO_LSTM_predictions_2017.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DAILY POLLUTANT PREDICTION WITH LSTM (SINGLE POLLUTANT)\n",
    "# NaNs are strictly ignored (no interpolation, no filling)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# FILE PATH (CHANGE THIS PER RUN)\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_NAME = \"NO\"   # for naming outputs only\n",
    "INPUT_FILE = r\"D:\\IPMA\\Results\\no_fire_meteo_Iberia.nc\"\n",
    "\n",
    "# ============================================================\n",
    "# USER SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_VAR = \"Mean\"\n",
    "TEST_YEAR = 2017\n",
    "TEST_MONTH = None        # None or 1–12\n",
    "LAG_DAYS = 14\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_VARS = [\n",
    "    \"Mean\",\n",
    "    \"temp_Max\",\n",
    "    \"wind_Max\",\n",
    "    \"precip_Total_Precipitation\",\n",
    "    \"frp_sum_Iberia\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def build_sequences(X, y, lags):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lags, len(X)):\n",
    "        X_out.append(X[i-lags:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "\n",
    "def train_mask(time):\n",
    "    return time.dt.year != TEST_YEAR\n",
    "\n",
    "\n",
    "def test_mask(time):\n",
    "    mask = time.dt.year == TEST_YEAR\n",
    "    if TEST_MONTH is not None:\n",
    "        mask = mask & (time.dt.month == TEST_MONTH)\n",
    "    return mask\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n==============================\")\n",
    "print(f\"Processing: {POLLUTANT_NAME}\")\n",
    "print(f\"==============================\")\n",
    "\n",
    "ds = xr.open_dataset(INPUT_FILE)\n",
    "\n",
    "# -------------------------------\n",
    "# CHECK VARIABLES\n",
    "# -------------------------------\n",
    "\n",
    "for var in INPUT_VARS:\n",
    "    if var not in ds:\n",
    "        raise KeyError(f\"{var} not found in {INPUT_FILE}\")\n",
    "\n",
    "y = ds[POLLUTANT_VAR]\n",
    "X = xr.merge([ds[var] for var in INPUT_VARS])\n",
    "\n",
    "time = ds.time\n",
    "train_idx = train_mask(time)\n",
    "test_idx = test_mask(time)\n",
    "\n",
    "X_train = X.sel(time=train_idx)\n",
    "y_train = y.sel(time=train_idx)\n",
    "X_test = X.sel(time=test_idx)\n",
    "y_test = y.sel(time=test_idx)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD TRAINING DATA (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for lat in ds.latitude.values:\n",
    "    for lon in ds.longitude.values:\n",
    "\n",
    "        X_ts = X_train.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y_train.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = (\n",
    "            ~np.isnan(y_ts) &\n",
    "            ~np.isnan(X_ts).any(axis=1)\n",
    "        )\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        if len(y_ts) <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_seq, y_seq = build_sequences(X_scaled, y_scaled, LAG_DAYS)\n",
    "\n",
    "        if len(y_seq) == 0:\n",
    "            continue\n",
    "\n",
    "        X_all.append(X_seq)\n",
    "        y_all.append(y_seq)\n",
    "\n",
    "X_train_all = np.concatenate(X_all)\n",
    "y_train_all = np.concatenate(y_all)\n",
    "\n",
    "print(f\"Training samples: {X_train_all.shape[0]}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL\n",
    "# ============================================================\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(LAG_DAYS, X_train_all.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_all,\n",
    "    y_train_all,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "test_times = time.sel(time=test_idx)\n",
    "\n",
    "preds = np.full(\n",
    "    (len(ds.latitude), len(ds.longitude), len(test_times)),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "for i, lat in enumerate(ds.latitude.values):\n",
    "    for j, lon in enumerate(ds.longitude.values):\n",
    "\n",
    "        X_ts = X.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = ~np.isnan(X_ts).any(axis=1)\n",
    "\n",
    "        if valid.sum() <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1))\n",
    "\n",
    "        test_start = np.where(test_idx.values)[0][0]\n",
    "\n",
    "        for k in range(len(test_times)):\n",
    "            t = test_start + k\n",
    "            if t - LAG_DAYS < 0:\n",
    "                continue\n",
    "\n",
    "            X_input = X_scaled[t-LAG_DAYS:t]\n",
    "\n",
    "            if np.isnan(X_input).any():\n",
    "                continue\n",
    "\n",
    "            pred_scaled = model.predict(\n",
    "                X_input.reshape(1, LAG_DAYS, -1),\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            preds[i, j, k] = scaler_y.inverse_transform(pred_scaled)[0, 0]\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "pred_da = xr.DataArray(\n",
    "    preds,\n",
    "    dims=(\"latitude\", \"longitude\", \"time\"),\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times\n",
    "    },\n",
    "    name=f\"{POLLUTANT_NAME}_predicted\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "error = pred_da - y_test\n",
    "\n",
    "mae = float(abs(error).where(np.isfinite(error)).mean())\n",
    "rmse = float(np.sqrt((error ** 2)).where(np.isfinite(error)).mean())\n",
    "bias = float(error.where(np.isfinite(error)).mean())\n",
    "corr = float(\n",
    "    xr.corr(pred_da, y_test, dim=\"time\")\n",
    "    .where(np.isfinite(pred_da))\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation (2017):\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  Bias: {bias:.4f}\")\n",
    "print(f\"  Corr: {corr:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS FOR MAPPING (NO PLOTTING)\n",
    "# ============================================================\n",
    "\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        f\"{POLLUTANT_NAME}_predicted\": pred_da,\n",
    "        f\"{POLLUTANT_NAME}_observed\": y_test,\n",
    "        f\"{POLLUTANT_NAME}_difference\": pred_da - y_test,\n",
    "    },\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add metadata (recommended)\n",
    "ds_out[f\"{POLLUTANT_NAME}_predicted\"].attrs[\"description\"] = \"LSTM predicted pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_observed\"].attrs[\"description\"] = \"Observed pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_difference\"].attrs[\"description\"] = \"Prediction minus observation\"\n",
    "\n",
    "ds_out.attrs[\"model\"] = \"LSTM\"\n",
    "ds_out.attrs[\"lags_days\"] = LAG_DAYS\n",
    "ds_out.attrs[\"test_year\"] = TEST_YEAR\n",
    "ds_out.attrs[\"pollutant\"] = POLLUTANT_NAME\n",
    "\n",
    "# Save to NetCDF\n",
    "output_file = f\"{POLLUTANT_NAME}_LSTM_predictions_{TEST_YEAR}.nc\"\n",
    "ds_out.to_netcdf(output_file)\n",
    "\n",
    "print(f\"\\nSaved results to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614f5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing: NO2\n",
      "==============================\n",
      "Training samples: 1139999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 0.0055\n",
      "Epoch 2/40\n",
      "\u001b[1m   28/35625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 4ms/step - loss: 0.0054 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 0.0053\n",
      "Epoch 3/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0052\n",
      "Epoch 4/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0052\n",
      "Epoch 5/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 0.0051\n",
      "Epoch 6/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 4ms/step - loss: 0.0051\n",
      "Epoch 7/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 6ms/step - loss: 0.0050\n",
      "Epoch 8/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 6ms/step - loss: 0.0050\n",
      "Epoch 9/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 4ms/step - loss: 0.0049\n",
      "Epoch 10/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0049\n",
      "Epoch 11/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0048\n",
      "Epoch 12/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0048\n",
      "Epoch 13/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0047\n",
      "Epoch 14/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0047\n",
      "Epoch 15/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 4ms/step - loss: 0.0047\n",
      "Epoch 16/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 6ms/step - loss: 0.0046\n",
      "Epoch 17/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 5ms/step - loss: 0.0046\n",
      "Epoch 18/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 4ms/step - loss: 0.0046\n",
      "Epoch 19/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0046\n",
      "Epoch 20/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0045\n",
      "Epoch 21/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0045\n",
      "Epoch 22/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0045\n",
      "Epoch 23/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0045\n",
      "Epoch 24/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0045\n",
      "Epoch 25/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0045\n",
      "Epoch 26/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0044\n",
      "Epoch 27/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0044\n",
      "Epoch 28/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0044\n",
      "Epoch 29/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0044\n",
      "Epoch 30/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0044\n",
      "Epoch 31/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 5ms/step - loss: 0.0044\n",
      "Epoch 32/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 6ms/step - loss: 0.0044\n",
      "Epoch 33/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 5ms/step - loss: 0.0044\n",
      "Epoch 34/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 6ms/step - loss: 0.0043\n",
      "Epoch 35/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 5ms/step - loss: 0.0043\n",
      "Epoch 36/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0043\n",
      "Epoch 37/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0043\n",
      "Epoch 38/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 4ms/step - loss: 0.0043\n",
      "Epoch 39/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0043\n",
      "Epoch 40/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 4ms/step - loss: 0.0043\n",
      "\n",
      "Evaluation (2017):\n",
      "  MAE:  1.7274\n",
      "  RMSE: 1.7274\n",
      "  Bias: -0.0086\n",
      "  Corr: 0.6964\n",
      "\n",
      "Saved results to: NO2_LSTM_predictions_2017.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DAILY POLLUTANT PREDICTION WITH LSTM (SINGLE POLLUTANT)\n",
    "# NaNs are strictly ignored (no interpolation, no filling)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# FILE PATH (CHANGE THIS PER RUN)\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_NAME = \"NO2\"   # for naming outputs only\n",
    "INPUT_FILE = r\"D:\\IPMA\\Results\\no2_fire_meteo_Iberia.nc\"\n",
    "\n",
    "# ============================================================\n",
    "# USER SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_VAR = \"Mean\"\n",
    "TEST_YEAR = 2017\n",
    "TEST_MONTH = None        # None or 1–12\n",
    "LAG_DAYS = 14\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_VARS = [\n",
    "    \"Mean\",\n",
    "    \"temp_Max\",\n",
    "    \"wind_Max\",\n",
    "    \"precip_Total_Precipitation\",\n",
    "    \"frp_sum_Iberia\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def build_sequences(X, y, lags):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lags, len(X)):\n",
    "        X_out.append(X[i-lags:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "\n",
    "def train_mask(time):\n",
    "    return time.dt.year != TEST_YEAR\n",
    "\n",
    "\n",
    "def test_mask(time):\n",
    "    mask = time.dt.year == TEST_YEAR\n",
    "    if TEST_MONTH is not None:\n",
    "        mask = mask & (time.dt.month == TEST_MONTH)\n",
    "    return mask\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n==============================\")\n",
    "print(f\"Processing: {POLLUTANT_NAME}\")\n",
    "print(f\"==============================\")\n",
    "\n",
    "ds = xr.open_dataset(INPUT_FILE)\n",
    "\n",
    "# -------------------------------\n",
    "# CHECK VARIABLES\n",
    "# -------------------------------\n",
    "\n",
    "for var in INPUT_VARS:\n",
    "    if var not in ds:\n",
    "        raise KeyError(f\"{var} not found in {INPUT_FILE}\")\n",
    "\n",
    "y = ds[POLLUTANT_VAR]\n",
    "X = xr.merge([ds[var] for var in INPUT_VARS])\n",
    "\n",
    "time = ds.time\n",
    "train_idx = train_mask(time)\n",
    "test_idx = test_mask(time)\n",
    "\n",
    "X_train = X.sel(time=train_idx)\n",
    "y_train = y.sel(time=train_idx)\n",
    "X_test = X.sel(time=test_idx)\n",
    "y_test = y.sel(time=test_idx)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD TRAINING DATA (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for lat in ds.latitude.values:\n",
    "    for lon in ds.longitude.values:\n",
    "\n",
    "        X_ts = X_train.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y_train.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = (\n",
    "            ~np.isnan(y_ts) &\n",
    "            ~np.isnan(X_ts).any(axis=1)\n",
    "        )\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        if len(y_ts) <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_seq, y_seq = build_sequences(X_scaled, y_scaled, LAG_DAYS)\n",
    "\n",
    "        if len(y_seq) == 0:\n",
    "            continue\n",
    "\n",
    "        X_all.append(X_seq)\n",
    "        y_all.append(y_seq)\n",
    "\n",
    "X_train_all = np.concatenate(X_all)\n",
    "y_train_all = np.concatenate(y_all)\n",
    "\n",
    "print(f\"Training samples: {X_train_all.shape[0]}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL\n",
    "# ============================================================\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(LAG_DAYS, X_train_all.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_all,\n",
    "    y_train_all,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "test_times = time.sel(time=test_idx)\n",
    "\n",
    "preds = np.full(\n",
    "    (len(ds.latitude), len(ds.longitude), len(test_times)),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "for i, lat in enumerate(ds.latitude.values):\n",
    "    for j, lon in enumerate(ds.longitude.values):\n",
    "\n",
    "        X_ts = X.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = ~np.isnan(X_ts).any(axis=1)\n",
    "\n",
    "        if valid.sum() <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1))\n",
    "\n",
    "        test_start = np.where(test_idx.values)[0][0]\n",
    "\n",
    "        for k in range(len(test_times)):\n",
    "            t = test_start + k\n",
    "            if t - LAG_DAYS < 0:\n",
    "                continue\n",
    "\n",
    "            X_input = X_scaled[t-LAG_DAYS:t]\n",
    "\n",
    "            if np.isnan(X_input).any():\n",
    "                continue\n",
    "\n",
    "            pred_scaled = model.predict(\n",
    "                X_input.reshape(1, LAG_DAYS, -1),\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            preds[i, j, k] = scaler_y.inverse_transform(pred_scaled)[0, 0]\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "pred_da = xr.DataArray(\n",
    "    preds,\n",
    "    dims=(\"latitude\", \"longitude\", \"time\"),\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times\n",
    "    },\n",
    "    name=f\"{POLLUTANT_NAME}_predicted\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "error = pred_da - y_test\n",
    "\n",
    "mae = float(abs(error).where(np.isfinite(error)).mean())\n",
    "rmse = float(np.sqrt((error ** 2)).where(np.isfinite(error)).mean())\n",
    "bias = float(error.where(np.isfinite(error)).mean())\n",
    "corr = float(\n",
    "    xr.corr(pred_da, y_test, dim=\"time\")\n",
    "    .where(np.isfinite(pred_da))\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation (2017):\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  Bias: {bias:.4f}\")\n",
    "print(f\"  Corr: {corr:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS FOR MAPPING (NO PLOTTING)\n",
    "# ============================================================\n",
    "\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        f\"{POLLUTANT_NAME}_predicted\": pred_da,\n",
    "        f\"{POLLUTANT_NAME}_observed\": y_test,\n",
    "        f\"{POLLUTANT_NAME}_difference\": pred_da - y_test,\n",
    "    },\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add metadata (recommended)\n",
    "ds_out[f\"{POLLUTANT_NAME}_predicted\"].attrs[\"description\"] = \"LSTM predicted pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_observed\"].attrs[\"description\"] = \"Observed pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_difference\"].attrs[\"description\"] = \"Prediction minus observation\"\n",
    "\n",
    "ds_out.attrs[\"model\"] = \"LSTM\"\n",
    "ds_out.attrs[\"lags_days\"] = LAG_DAYS\n",
    "ds_out.attrs[\"test_year\"] = TEST_YEAR\n",
    "ds_out.attrs[\"pollutant\"] = POLLUTANT_NAME\n",
    "\n",
    "# Save to NetCDF\n",
    "output_file = f\"{POLLUTANT_NAME}_LSTM_predictions_{TEST_YEAR}.nc\"\n",
    "ds_out.to_netcdf(output_file)\n",
    "\n",
    "print(f\"\\nSaved results to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23583837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing: PM2.5\n",
      "==============================\n",
      "Training samples: 1139997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 4ms/step - loss: 0.0013\n",
      "Epoch 2/40\n",
      "\u001b[1m   11/35625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:04\u001b[0m 5ms/step - loss: 0.0037  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 5ms/step - loss: 0.0013\n",
      "Epoch 3/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 4/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 5/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 6ms/step - loss: 0.0012\n",
      "Epoch 6/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 7/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 8/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 9/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 10/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 11/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 5ms/step - loss: 0.0012\n",
      "Epoch 12/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 13/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 14/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 15/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 16/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 17/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 18/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 19/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 20/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 21/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 22/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 6ms/step - loss: 0.0010\n",
      "Epoch 23/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 6ms/step - loss: 0.0010\n",
      "Epoch 24/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 25/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 26/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 27/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 5ms/step - loss: 9.9882e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - loss: 9.9286e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 5ms/step - loss: 9.8700e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - loss: 9.7853e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 5ms/step - loss: 9.7297e-04\n",
      "Epoch 32/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 5ms/step - loss: 9.6671e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 5ms/step - loss: 9.6258e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - loss: 9.5489e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 5ms/step - loss: 9.5087e-04\n",
      "Epoch 36/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 5ms/step - loss: 9.4666e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 5ms/step - loss: 9.4186e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 5ms/step - loss: 9.3753e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 5ms/step - loss: 9.3351e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 6ms/step - loss: 9.3061e-04\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DAILY POLLUTANT PREDICTION WITH LSTM (SINGLE POLLUTANT)\n",
    "# NaNs are strictly ignored (no interpolation, no filling)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# FILE PATH (CHANGE THIS PER RUN)\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_NAME = \"PM2.5\"   # for naming outputs only\n",
    "INPUT_FILE = r\"D:\\IPMA\\Results\\pm2p5_fire_meteo_Iberia.nc\"\n",
    "\n",
    "# ============================================================\n",
    "# USER SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_VAR = \"Mean\"\n",
    "TEST_YEAR = 2017\n",
    "TEST_MONTH = None        # None or 1–12\n",
    "LAG_DAYS = 14\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_VARS = [\n",
    "    \"Mean\",\n",
    "    \"temp_Max\",\n",
    "    \"wind_Max\",\n",
    "    \"precip_Total_Precipitation\",\n",
    "    \"frp_sum_Iberia\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def build_sequences(X, y, lags):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lags, len(X)):\n",
    "        X_out.append(X[i-lags:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "\n",
    "def train_mask(time):\n",
    "    return time.dt.year != TEST_YEAR\n",
    "\n",
    "\n",
    "def test_mask(time):\n",
    "    mask = time.dt.year == TEST_YEAR\n",
    "    if TEST_MONTH is not None:\n",
    "        mask = mask & (time.dt.month == TEST_MONTH)\n",
    "    return mask\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n==============================\")\n",
    "print(f\"Processing: {POLLUTANT_NAME}\")\n",
    "print(f\"==============================\")\n",
    "\n",
    "ds = xr.open_dataset(INPUT_FILE)\n",
    "\n",
    "# -------------------------------\n",
    "# CHECK VARIABLES\n",
    "# -------------------------------\n",
    "\n",
    "for var in INPUT_VARS:\n",
    "    if var not in ds:\n",
    "        raise KeyError(f\"{var} not found in {INPUT_FILE}\")\n",
    "\n",
    "y = ds[POLLUTANT_VAR]\n",
    "X = xr.merge([ds[var] for var in INPUT_VARS])\n",
    "\n",
    "time = ds.time\n",
    "train_idx = train_mask(time)\n",
    "test_idx = test_mask(time)\n",
    "\n",
    "X_train = X.sel(time=train_idx)\n",
    "y_train = y.sel(time=train_idx)\n",
    "X_test = X.sel(time=test_idx)\n",
    "y_test = y.sel(time=test_idx)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD TRAINING DATA (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for lat in ds.latitude.values:\n",
    "    for lon in ds.longitude.values:\n",
    "\n",
    "        X_ts = X_train.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y_train.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = (\n",
    "            ~np.isnan(y_ts) &\n",
    "            ~np.isnan(X_ts).any(axis=1)\n",
    "        )\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        if len(y_ts) <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_seq, y_seq = build_sequences(X_scaled, y_scaled, LAG_DAYS)\n",
    "\n",
    "        if len(y_seq) == 0:\n",
    "            continue\n",
    "\n",
    "        X_all.append(X_seq)\n",
    "        y_all.append(y_seq)\n",
    "\n",
    "X_train_all = np.concatenate(X_all)\n",
    "y_train_all = np.concatenate(y_all)\n",
    "\n",
    "print(f\"Training samples: {X_train_all.shape[0]}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL\n",
    "# ============================================================\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(LAG_DAYS, X_train_all.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_all,\n",
    "    y_train_all,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "test_times = time.sel(time=test_idx)\n",
    "\n",
    "preds = np.full(\n",
    "    (len(ds.latitude), len(ds.longitude), len(test_times)),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "for i, lat in enumerate(ds.latitude.values):\n",
    "    for j, lon in enumerate(ds.longitude.values):\n",
    "\n",
    "        X_ts = X.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = ~np.isnan(X_ts).any(axis=1)\n",
    "\n",
    "        if valid.sum() <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1))\n",
    "\n",
    "        test_start = np.where(test_idx.values)[0][0]\n",
    "\n",
    "        for k in range(len(test_times)):\n",
    "            t = test_start + k\n",
    "            if t - LAG_DAYS < 0:\n",
    "                continue\n",
    "\n",
    "            X_input = X_scaled[t-LAG_DAYS:t]\n",
    "\n",
    "            if np.isnan(X_input).any():\n",
    "                continue\n",
    "\n",
    "            pred_scaled = model.predict(\n",
    "                X_input.reshape(1, LAG_DAYS, -1),\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            preds[i, j, k] = scaler_y.inverse_transform(pred_scaled)[0, 0]\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "pred_da = xr.DataArray(\n",
    "    preds,\n",
    "    dims=(\"latitude\", \"longitude\", \"time\"),\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times\n",
    "    },\n",
    "    name=f\"{POLLUTANT_NAME}_predicted\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "error = pred_da - y_test\n",
    "\n",
    "mae = float(abs(error).where(np.isfinite(error)).mean())\n",
    "rmse = float(np.sqrt((error ** 2)).where(np.isfinite(error)).mean())\n",
    "bias = float(error.where(np.isfinite(error)).mean())\n",
    "corr = float(\n",
    "    xr.corr(pred_da, y_test, dim=\"time\")\n",
    "    .where(np.isfinite(pred_da))\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation (2017):\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  Bias: {bias:.4f}\")\n",
    "print(f\"  Corr: {corr:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS FOR MAPPING (NO PLOTTING)\n",
    "# ============================================================\n",
    "\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        f\"{POLLUTANT_NAME}_predicted\": pred_da,\n",
    "        f\"{POLLUTANT_NAME}_observed\": y_test,\n",
    "        f\"{POLLUTANT_NAME}_difference\": pred_da - y_test,\n",
    "    },\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add metadata (recommended)\n",
    "ds_out[f\"{POLLUTANT_NAME}_predicted\"].attrs[\"description\"] = \"LSTM predicted pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_observed\"].attrs[\"description\"] = \"Observed pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_difference\"].attrs[\"description\"] = \"Prediction minus observation\"\n",
    "\n",
    "ds_out.attrs[\"model\"] = \"LSTM\"\n",
    "ds_out.attrs[\"lags_days\"] = LAG_DAYS\n",
    "ds_out.attrs[\"test_year\"] = TEST_YEAR\n",
    "ds_out.attrs[\"pollutant\"] = POLLUTANT_NAME\n",
    "\n",
    "# Save to NetCDF\n",
    "output_file = f\"{POLLUTANT_NAME}_LSTM_predictions_{TEST_YEAR}.nc\"\n",
    "ds_out.to_netcdf(output_file)\n",
    "\n",
    "print(f\"\\nSaved results to: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Processing: PM10\n",
      "==============================\n",
      "Training samples: 1139998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0014\n",
      "Epoch 2/40\n",
      "\u001b[1m   31/35625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 3ms/step - loss: 0.0016     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\keras\\src\\callbacks\\early_stopping.py:99: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0013\n",
      "Epoch 3/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0013\n",
      "Epoch 4/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0013\n",
      "Epoch 5/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0013\n",
      "Epoch 6/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0013\n",
      "Epoch 7/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 8/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 9/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 10/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 11/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 12/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 13/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 14/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 15/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0012\n",
      "Epoch 16/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 17/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 18/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 19/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 20/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 21/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 22/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 4ms/step - loss: 0.0011\n",
      "Epoch 23/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 24/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 25/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 6ms/step - loss: 0.0011\n",
      "Epoch 26/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 5ms/step - loss: 0.0011\n",
      "Epoch 27/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 28/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 4ms/step - loss: 0.0010\n",
      "Epoch 29/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 30/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 31/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 32/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 33/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 34/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 5ms/step - loss: 0.0010\n",
      "Epoch 35/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 5ms/step - loss: 0.0010  \n",
      "Epoch 36/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 5ms/step - loss: 9.9628e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 5ms/step - loss: 9.9375e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 5ms/step - loss: 9.8944e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 5ms/step - loss: 9.8575e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m35625/35625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 5ms/step - loss: 9.8026e-04\n",
      "\n",
      "Evaluation (2017):\n",
      "  MAE:  7.1498\n",
      "  RMSE: 7.1498\n",
      "  Bias: 0.6430\n",
      "  Corr: 0.5988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\sofia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:2019: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DAILY POLLUTANT PREDICTION WITH LSTM (SINGLE POLLUTANT)\n",
    "# NaNs are strictly ignored (no interpolation, no filling)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# FILE PATH (CHANGE THIS PER RUN)\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_NAME = \"PM10\"   # for naming outputs only\n",
    "INPUT_FILE = r\"D:\\IPMA\\Results\\pm10_fire_meteo_Iberia.nc\"\n",
    "\n",
    "# ============================================================\n",
    "# USER SETTINGS\n",
    "# ============================================================\n",
    "\n",
    "POLLUTANT_VAR = \"Mean\"\n",
    "TEST_YEAR = 2017\n",
    "TEST_MONTH = None        # None or 1–12\n",
    "LAG_DAYS = 14\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INPUT_VARS = [\n",
    "    \"Mean\",\n",
    "    \"temp_Max\",\n",
    "    \"wind_Max\",\n",
    "    \"precip_Total_Precipitation\",\n",
    "    \"frp_sum_Iberia\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def build_sequences(X, y, lags):\n",
    "    X_out, y_out = [], []\n",
    "    for i in range(lags, len(X)):\n",
    "        X_out.append(X[i-lags:i])\n",
    "        y_out.append(y[i])\n",
    "    return np.array(X_out), np.array(y_out)\n",
    "\n",
    "\n",
    "def train_mask(time):\n",
    "    return time.dt.year != TEST_YEAR\n",
    "\n",
    "\n",
    "def test_mask(time):\n",
    "    mask = time.dt.year == TEST_YEAR\n",
    "    if TEST_MONTH is not None:\n",
    "        mask = mask & (time.dt.month == TEST_MONTH)\n",
    "    return mask\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n==============================\")\n",
    "print(f\"Processing: {POLLUTANT_NAME}\")\n",
    "print(f\"==============================\")\n",
    "\n",
    "ds = xr.open_dataset(INPUT_FILE)\n",
    "\n",
    "# -------------------------------\n",
    "# CHECK VARIABLES\n",
    "# -------------------------------\n",
    "\n",
    "for var in INPUT_VARS:\n",
    "    if var not in ds:\n",
    "        raise KeyError(f\"{var} not found in {INPUT_FILE}\")\n",
    "\n",
    "y = ds[POLLUTANT_VAR]\n",
    "X = xr.merge([ds[var] for var in INPUT_VARS])\n",
    "\n",
    "time = ds.time\n",
    "train_idx = train_mask(time)\n",
    "test_idx = test_mask(time)\n",
    "\n",
    "X_train = X.sel(time=train_idx)\n",
    "y_train = y.sel(time=train_idx)\n",
    "X_test = X.sel(time=test_idx)\n",
    "y_test = y.sel(time=test_idx)\n",
    "\n",
    "# ============================================================\n",
    "# BUILD TRAINING DATA (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "for lat in ds.latitude.values:\n",
    "    for lon in ds.longitude.values:\n",
    "\n",
    "        X_ts = X_train.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y_train.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = (\n",
    "            ~np.isnan(y_ts) &\n",
    "            ~np.isnan(X_ts).any(axis=1)\n",
    "        )\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        if len(y_ts) <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1)).ravel()\n",
    "\n",
    "        X_seq, y_seq = build_sequences(X_scaled, y_scaled, LAG_DAYS)\n",
    "\n",
    "        if len(y_seq) == 0:\n",
    "            continue\n",
    "\n",
    "        X_all.append(X_seq)\n",
    "        y_all.append(y_seq)\n",
    "\n",
    "X_train_all = np.concatenate(X_all)\n",
    "y_train_all = np.concatenate(y_all)\n",
    "\n",
    "print(f\"Training samples: {X_train_all.shape[0]}\")\n",
    "\n",
    "# ============================================================\n",
    "# MODEL\n",
    "# ============================================================\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(LAG_DAYS, X_train_all.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_all,\n",
    "    y_train_all,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# PREDICTION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "test_times = time.sel(time=test_idx)\n",
    "\n",
    "preds = np.full(\n",
    "    (len(ds.latitude), len(ds.longitude), len(test_times)),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "for i, lat in enumerate(ds.latitude.values):\n",
    "    for j, lon in enumerate(ds.longitude.values):\n",
    "\n",
    "        X_ts = X.sel(latitude=lat, longitude=lon).to_array().values.T\n",
    "        y_ts = y.sel(latitude=lat, longitude=lon).values\n",
    "\n",
    "        valid = ~np.isnan(X_ts).any(axis=1)\n",
    "\n",
    "        if valid.sum() <= LAG_DAYS:\n",
    "            continue\n",
    "\n",
    "        X_ts = X_ts[valid]\n",
    "        y_ts = y_ts[valid]\n",
    "\n",
    "        scaler_X = MinMaxScaler()\n",
    "        scaler_y = MinMaxScaler()\n",
    "\n",
    "        X_scaled = scaler_X.fit_transform(X_ts)\n",
    "        y_scaled = scaler_y.fit_transform(y_ts.reshape(-1, 1))\n",
    "\n",
    "        test_start = np.where(test_idx.values)[0][0]\n",
    "\n",
    "        for k in range(len(test_times)):\n",
    "            t = test_start + k\n",
    "            if t - LAG_DAYS < 0:\n",
    "                continue\n",
    "\n",
    "            X_input = X_scaled[t-LAG_DAYS:t]\n",
    "\n",
    "            if np.isnan(X_input).any():\n",
    "                continue\n",
    "\n",
    "            pred_scaled = model.predict(\n",
    "                X_input.reshape(1, LAG_DAYS, -1),\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            preds[i, j, k] = scaler_y.inverse_transform(pred_scaled)[0, 0]\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "pred_da = xr.DataArray(\n",
    "    preds,\n",
    "    dims=(\"latitude\", \"longitude\", \"time\"),\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times\n",
    "    },\n",
    "    name=f\"{POLLUTANT_NAME}_predicted\"\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION (NaN-SAFE)\n",
    "# ============================================================\n",
    "\n",
    "error = pred_da - y_test\n",
    "\n",
    "mae = float(abs(error).where(np.isfinite(error)).mean())\n",
    "rmse = float(np.sqrt((error ** 2)).where(np.isfinite(error)).mean())\n",
    "bias = float(error.where(np.isfinite(error)).mean())\n",
    "corr = float(\n",
    "    xr.corr(pred_da, y_test, dim=\"time\")\n",
    "    .where(np.isfinite(pred_da))\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation (2017):\")\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  Bias: {bias:.4f}\")\n",
    "print(f\"  Corr: {corr:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULTS FOR MAPPING (NO PLOTTING)\n",
    "# ============================================================\n",
    "\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        f\"{POLLUTANT_NAME}_predicted\": pred_da,\n",
    "        f\"{POLLUTANT_NAME}_observed\": y_test,\n",
    "        f\"{POLLUTANT_NAME}_difference\": pred_da - y_test,\n",
    "    },\n",
    "    coords={\n",
    "        \"latitude\": ds.latitude,\n",
    "        \"longitude\": ds.longitude,\n",
    "        \"time\": test_times,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add metadata (recommended)\n",
    "ds_out[f\"{POLLUTANT_NAME}_predicted\"].attrs[\"description\"] = \"LSTM predicted pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_observed\"].attrs[\"description\"] = \"Observed pollutant concentration\"\n",
    "ds_out[f\"{POLLUTANT_NAME}_difference\"].attrs[\"description\"] = \"Prediction minus observation\"\n",
    "\n",
    "ds_out.attrs[\"model\"] = \"LSTM\"\n",
    "ds_out.attrs[\"lags_days\"] = LAG_DAYS\n",
    "ds_out.attrs[\"test_year\"] = TEST_YEAR\n",
    "ds_out.attrs[\"pollutant\"] = POLLUTANT_NAME\n",
    "\n",
    "# Save to NetCDF\n",
    "output_file = f\"{POLLUTANT_NAME}_LSTM_predictions_{TEST_YEAR}.nc\"\n",
    "ds_out.to_netcdf(output_file)\n",
    "\n",
    "print(f\"\\nSaved results to: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
