{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERA5 - uv_wind, temp, precip\n",
    "\n",
    "Wind speed and direction; Temp conver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind/Temp/Precip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separates files by year when downloading more than 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To separate year by year files downloaded with more than 1 year each - DONE for 1979-2024\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\ERA5\\UV_wind\\ERA5_hourly_uv_2003_1999.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Destination folder to save yearly files\n",
    "dest_folder = r\"E:\\IPMA\\ERA5\\UV_wind\"\n",
    "\n",
    "# Iterate through years\n",
    "for year in years:\n",
    "    # Filter dataset for the given year\n",
    "    yearly_ds = ds.sel(valid_time=ds.valid_time.dt.year == year)\n",
    "\n",
    "    if yearly_ds.valid_time.size > 0:  # Only save if data exists for the year\n",
    "        output_filename = rf\"{dest_folder}\\ERA5_hourly_uv_{year}.nc\" #change accordingly to what file is being used\n",
    "        yearly_ds.to_netcdf(output_filename)\n",
    "        print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separates files by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 1979-2024\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Directory where all .nc files are located\n",
    "file_path = r\"E:\\IPMA\\ERA5\\UV_wind\\1raw_year_1979_2024\"\n",
    "\n",
    "# Destination folder for monthly files\n",
    "dest_folder = r\"E:\\IPMA\\ERA5\\UV_wind\\1raw_month_1979_2024\"\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(file_path):\n",
    "    if filename.endswith(\".nc\"):\n",
    "        file_full_path = os.path.join(file_path, filename)\n",
    "        \n",
    "        # Load the dataset\n",
    "        ds = xr.open_dataset(file_full_path)\n",
    "\n",
    "        # Ensure valid_time is a datetime object\n",
    "        ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "        # Get unique years in the dataset\n",
    "        years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "        # Iterate through years and months\n",
    "        for year in years:\n",
    "            for month in range(1, 13):\n",
    "                # Filter dataset for the given year and month\n",
    "                monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "                if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "                    output_filename = rf\"{dest_folder}\\ERA5_hourly_uv_{year}{month:02d}.nc\" #change accordingly to what file is being used\n",
    "                    monthly_ds.to_netcdf(output_filename)\n",
    "                    print(f\"Saved {output_filename}\")\n",
    "\n",
    "        # Close the dataset\n",
    "        ds.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change Kelvin to Celsius - DONE for temperature\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the folder containing the NetCDF files\n",
    "data_dir = r\"E:\\IPMA\\ERA5\\Temperature\\1raw_year_1979_2024\"\n",
    "# Define the folder to save the converted files\n",
    "output_dir = r\"E:\\IPMA\\ERA5\\Temperature\\2conversion_year_1979_2024\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the directory\n",
    "nc_files = glob.glob(os.path.join(data_dir, \"*.nc\"))\n",
    "\n",
    "# Loop through each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Check if 't2m' exists and convert it from Kelvin to Celsius\n",
    "    if \"t2m\" in ds:\n",
    "        ds[\"t2m\"] = ds[\"t2m\"] - 273.15  # Convert to Celsius\n",
    "        ds[\"t2m\"].attrs[\"units\"] = \"Celsius\"  # Update metadata\n",
    "\n",
    "        # Define output file path\n",
    "        output_file_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "        \n",
    "        # Save the updated dataset to the new folder\n",
    "        ds.to_netcdf(output_file_path, mode=\"w\")\n",
    "\n",
    "        print(f\"Converted 't2m' to Celsius and saved to {output_file_path}\")\n",
    "    else:\n",
    "        print(f\"Skipping {file_path}, 't2m' variable not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate wind speed and direction based on u&v component\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define folders\n",
    "input_folder = r\"D:\\IPMA\\ERA5\\UV_wind\\1raw_year_1979_2024\"\n",
    "output_folder = r\"D:\\IPMA\\ERA5\\UV_wind\\2wind_speed_direction\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Find all relevant NetCDF files\n",
    "nc_files = sorted(glob(os.path.join(input_folder, \"ERA5_hourly_uv_*.nc\")))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Calculate wind speed\n",
    "    wind_speed = np.sqrt(ds['u10']**2 + ds['v10']**2)\n",
    "\n",
    "    # Calculate wind direction (degrees, meteorological convention)\n",
    "    wind_dir = (180 + np.degrees(np.arctan2(ds['u10'], ds['v10']))) % 360\n",
    "\n",
    "    # Add to dataset\n",
    "    ds = ds.assign(wind_speed=wind_speed, wind_direction=wind_dir)\n",
    "\n",
    "    # Add metadata\n",
    "    ds['wind_speed'].attrs['units'] = 'm/s'\n",
    "    ds['wind_speed'].attrs['description'] = '10m wind speed calculated from u10 and v10'\n",
    "    ds['wind_direction'].attrs['units'] = 'degrees'\n",
    "    ds['wind_direction'].attrs['description'] = 'Wind direction (from which wind blows, 0°=North, clockwise)'\n",
    "\n",
    "    # Create output filename, e.g., ERA5_hourly_wind_1979.nc\n",
    "    year_str = os.path.basename(file_path).split('_')[-1].split('.')[0]\n",
    "    out_filename = f\"ERA5_hourly_wind_{year_str}.nc\"\n",
    "    out_path = os.path.join(output_folder, out_filename)\n",
    "\n",
    "    # Save only the wind_speed and wind_direction variables (optional)\n",
    "    ds[['wind_speed', 'wind_direction']].to_netcdf(out_path)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "print(\"✅ All files processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert .npy to .nc files - RAQUEL SOURCE - DONE\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing .npy files\n",
    "input_folder = r\"E:\\IPMA\\SPI\\SPI3\"\n",
    "output_folder = r\"E:\\IPMA\\SPI\\SPI3\\nc\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all .npy files in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".npy\"):  # Process only .npy files\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        data = np.load(file_path)  # Load the 3D array\n",
    "\n",
    "        # Define dimensions (adjust accordingly)\n",
    "        dims = (\"time\", \"lat\", \"lon\")  # Change based on data structure\n",
    "        coords = {\n",
    "            \"time\": np.arange(data.shape[0]),  # Modify based on actual data\n",
    "            \"lat\": np.linspace(34, 66, data.shape[1]),  # Modify latitudes\n",
    "            \"lon\": np.linspace(-12, 36, data.shape[2])  # Modify longitudes\n",
    "        }\n",
    "\n",
    "        # Convert to xarray DataArray\n",
    "        da = xr.DataArray(data, dims=dims, coords=coords, name=\"spi03\")\n",
    "\n",
    "        # Convert to Dataset\n",
    "        ds = da.to_dataset(name=\"spi03\")\n",
    "\n",
    "        # Save as NetCDF file\n",
    "        output_filename = filename.replace(\".npy\", \".nc\")\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Converted {filename} -> {output_filename}\")\n",
    "\n",
    "print(\"Batch conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate spei for 1979-2024 - WEB SOURCE - DONE\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "def extract_spei_data(input_file, start_year=1979, end_year=2024):\n",
    "    \"\"\"\n",
    "    Extracts data from a NetCDF file for the years between start_year and end_year and saves it to a new NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input NetCDF file.\n",
    "        start_year (int): Start year for extraction (default 1979).\n",
    "        end_year (int): End year for extraction (default 2024).\n",
    "    \"\"\"\n",
    "    # Open the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    # Ensure time is in datetime format (if it's not already in datetime format)\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "\n",
    "    # Filter the data based on the time dimension (between the start and end year)\n",
    "    filtered_ds = ds.sel(time=slice(f\"{start_year}-01-01\", f\"{end_year}-12-31\"))\n",
    "    \n",
    "    # Create output filename dynamically based on the input file name and year range\n",
    "    base_filename = input_file.split('/')[-1].split('.')[0]  # Extract file name (e.g., spei_01)\n",
    "    output_file = f\"{base_filename}_{start_year}-{end_year}.nc\"\n",
    "    \n",
    "    # Save the filtered dataset to a new NetCDF file\n",
    "    filtered_ds.to_netcdf(output_file)\n",
    "    print(f\"Saved filtered data to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\spei12.nc\"\n",
    "extract_spei_data(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate spei for each year between 1979-2024 - WEB SOURCE - DONE\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "def extract_spei_data_by_year(input_file, start_year=1979, end_year=2024):\n",
    "    \"\"\"\n",
    "    Extracts data from a NetCDF file for each year between start_year and end_year\n",
    "    and saves each year as a separate NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input NetCDF file.\n",
    "        start_year (int): Start year for extraction (default 1979).\n",
    "        end_year (int): End year for extraction (default 2024).\n",
    "    \"\"\"\n",
    "    # Open the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    # Ensure time is in datetime format (if it's not already in datetime format)\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "\n",
    "    # Loop through each year and extract the data for that year\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Filter the dataset for the current year\n",
    "        filtered_ds = ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "        \n",
    "        # Create output filename for each year\n",
    "        base_filename = input_file.split('/')[-1].split('.')[0]  # Extract file name (e.g., spei_01)\n",
    "        output_file = f\"{base_filename}_{year}.nc\"\n",
    "        \n",
    "        # Save the filtered data for this year to a new NetCDF file\n",
    "        filtered_ds.to_netcdf(output_file)\n",
    "        print(f\"Saved filtered data for {year} to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\spei12.nc\"\n",
    "extract_spei_data_by_year(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - WEB SOURCE - DONE for 1979-2023\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# Define your input and output directories\n",
    "input_dir = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\1raw_1979_2023\"\n",
    "output_dir = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\2cropped_1979_2023\"\n",
    "\n",
    "# Define the latitude and longitude boundaries for your study area\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36  # Study area for Europe\n",
    "\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through all NC files in the input directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".nc\"):  # Check if it's a NetCDF file\n",
    "        input_file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "        # Open the NetCDF file using xarray\n",
    "        with xr.open_dataset(input_file_path) as ds:\n",
    "            # Crop the dataset to include only the specified region\n",
    "            ds_europe = ds.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))\n",
    "\n",
    "            # Create the output file path\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "            # Save the cropped data to a new NetCDF file\n",
    "            ds_europe.to_netcdf(output_file_path)\n",
    "            print(f\"Saved cropped file: {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
