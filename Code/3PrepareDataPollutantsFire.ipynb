{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pollutants PM10 & PM2.5, NO2, CO2, O3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMS global reanalysis EAC4 - Data Preparation (according to what Virgilio gave in matlab scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\CAMS\\chem_singlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_singlvl_2024.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Iterate through years and months\n",
    "dest_folder = r\"E:\\IPMA\\CAMS\\chem_singlvl\\0raw_2023_2024\"\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        # Filter dataset for the given year and month\n",
    "        monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "        if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "            output_filename = rf\"{dest_folder}\\CAMS_global_reanalysis_EAC4_chem_singlvl_{year}{month:02d}.nc\"\n",
    "            monthly_ds.to_netcdf(output_filename)\n",
    "            print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change valid_time to time (so it matches) - DONE for 2023-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to your directory containing the NetCDF files\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\0raw_2023_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\1raw_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Loop through each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Check if 'valid_time' exists and rename it to 'time'\n",
    "    if 'valid_time' in ds:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "    # Get the filename and create the output path in the new directory\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the updated dataset to the output directory (overwrite original file in the new folder)\n",
    "    ds.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Renamed 'valid_time' to 'time' and saved to {output_file}\")\n",
    "\n",
    "print(\"Renaming and saving to new folder complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert coords from 0 to 360 to -180 to 180 - DONE for 2003-2024 again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\1raw_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\2transformation_2003_2024\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of NetCDF files in the input folder\n",
    "nc_files = [f for f in os.listdir(input_folder) if f.endswith(\".nc\")]\n",
    "\n",
    "# Process each file\n",
    "for nc_file in nc_files:\n",
    "    input_path = os.path.join(input_folder, nc_file)\n",
    "    output_path = os.path.join(output_folder, nc_file)\n",
    "\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Extract longitude and latitude\n",
    "    lon = ds['longitude'].values  # (480,)\n",
    "    lat = ds['latitude'].values   # (241,)\n",
    "\n",
    "    # Convert longitude from 0-360 to -180 to 180\n",
    "    lon2 = (lon + 180) % 360 - 180\n",
    "\n",
    "    # Swap the first and second halves\n",
    "    lon3 = np.copy(lon2)\n",
    "    lon3[:240] = lon2[240:480]\n",
    "    lon3[240:480] = lon2[:240]\n",
    "\n",
    "    # Create a meshgrid (not strictly needed for saving, but useful)\n",
    "    LON, LAT = np.meshgrid(lon3, lat)\n",
    "\n",
    "    # Apply the same transformation to pm10, pm1, and pm2p5\n",
    "    for var in [\"pm10\", \"pm1\", \"pm2p5\"]:\n",
    "        if var in ds:\n",
    "            data = ds[var].values  # Shape: (time, lat, lon)\n",
    "            transformed_data = np.copy(data)\n",
    "\n",
    "            # Swap the longitude axis (last axis)\n",
    "            transformed_data[:, :, :240] = data[:, :, 240:480]\n",
    "            transformed_data[:, :, 240:480] = data[:, :, :240]\n",
    "\n",
    "            # Replace the dataset variable with the corrected data\n",
    "            ds[var].values = transformed_data\n",
    "\n",
    "    # Update longitude in the dataset\n",
    "    ds = ds.assign_coords(longitude=lon3)\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Saved transformed file: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - DONE for 2003-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define your study area (lat_max, lon_min, lat_min, lon_max)\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\2transformation_2003_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\3cropped_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Ensure latitude slicing is correct (since it decreases from 90 to -90)\n",
    "    ds_cropped = ds.sel(\n",
    "        latitude=slice(lat_max, lat_min),  # lat_max is greater than lat_min\n",
    "        longitude=slice(lon_min, lon_max)  # lon_min is less than lon_max\n",
    "    )\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the cropped dataset\n",
    "    ds_cropped.to_netcdf(output_file)\n",
    "    print(f\"Saved cropped file to {output_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert PM from kg/m3 to ug/m3 - DONE again\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define input and output directories\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\3cropped_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\4conversion_2003_2024\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List of NetCDF files\n",
    "nc_files = glob.glob(os.path.join(input_folder, \"*.nc\"))\n",
    "\n",
    "for nc_file in nc_files:\n",
    "    try:\n",
    "        # Open the NetCDF file\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "\n",
    "        # Convert PM values to µg/m³\n",
    "        ds[\"pm1\"] = ds[\"pm1\"] * 1e9\n",
    "        ds[\"pm2p5\"] = ds[\"pm2p5\"] * 1e9\n",
    "        ds[\"pm10\"] = ds[\"pm10\"] * 1e9\n",
    "\n",
    "        # Get the filename and construct output path\n",
    "        filename = os.path.basename(nc_file)  # Extracts filename only\n",
    "        output_file = os.path.join(output_folder, filename)  # Keeps same filename in new folder\n",
    "\n",
    "        # Save the modified NetCDF file in the new folder\n",
    "        ds.to_netcdf(output_file)\n",
    "        ds.close()  # Close the dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {nc_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put together files for each year - DONE for 2003-2024 (final step) again\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the original path where the files are located\n",
    "input_path = r\"D:\\IPMA\\CAMS\\chem_singlvl\\4conversion_2003_2024\"\n",
    "\n",
    "# Define the new path where you want to save the compiled files\n",
    "output_path_base = r\"D:\\IPMA\\CAMS\\chem_singlvl\\5compile_2003_2024\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_path_base, exist_ok=True)\n",
    "\n",
    "# Loop over each year from 2003 to 2024\n",
    "for year in range(2003, 2025):\n",
    "    # Create the file pattern for the specific year (e.g., 2003*)\n",
    "    file_pattern = os.path.join(input_path, f\"CAMS_global_reanalysis_EAC4_chem_singlvl_{year}*.nc\")\n",
    "    \n",
    "    # Use glob to find all .nc files for the specified year\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if files:  # Only proceed if there are files for that year\n",
    "        # Open all files for the year and concatenate along the 'time' dimension\n",
    "        ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "\n",
    "        # Save the compiled dataset to the new path\n",
    "        output_path = os.path.join(output_path_base, f\"CAMS_global_reanalysis_EAC4_chem_singlvl_{year}.nc\")\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Files for {year} have been compiled into: {output_path}\")\n",
    "    else:\n",
    "        print(f\"No files found for {year}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_2024.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Iterate through years and months\n",
    "dest_folder = r\"E:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\"\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        # Filter dataset for the given year and month\n",
    "        monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "        if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "            output_filename = rf\"{dest_folder}\\CAMS_global_reanalysis_EAC4_chem_multlvl_{year}{month:02d}.nc\"\n",
    "            monthly_ds.to_netcdf(output_filename)\n",
    "            print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15100\\494470751.py:29: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file to: D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15100\\494470751.py:29: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file to: D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n",
      "✅ All files processed!\n"
     ]
    }
   ],
   "source": [
    "# To change valid_time to time + remove pressure_level if present - DONE for 2023–2024 files again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Process each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Rename 'valid_time' to 'time' if needed\n",
    "    if 'valid_time' in ds:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "    # Handle pressure_level if it's a dimension of size 1\n",
    "    if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n",
    "        ds = ds.isel(pressure_level=0).squeeze(drop=True)\n",
    "\n",
    "    # Drop pressure_level variable if it's still around (not a dimension)\n",
    "    if \"pressure_level\" in ds.variables and \"pressure_level\" not in ds.dims:\n",
    "        ds = ds.drop_vars(\"pressure_level\")\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    ds.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Saved cleaned file to: {output_file}\")\n",
    "\n",
    "print(\"✅ All files processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Saved transformed file: D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Processing: CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n",
      "Saved transformed file: D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n"
     ]
    }
   ],
   "source": [
    "# To convert coords from 0 to 360 to -180 to 180 - DONE for 2003-2024 again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of NetCDF files in the input folder\n",
    "nc_files = [f for f in os.listdir(input_folder) if f.endswith(\".nc\")]\n",
    "\n",
    "# Process each file\n",
    "for nc_file in nc_files:\n",
    "    input_path = os.path.join(input_folder, nc_file)\n",
    "    output_path = os.path.join(output_folder, nc_file)\n",
    "\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Extract longitude and latitude\n",
    "    lon = ds['longitude'].values  # (480,)\n",
    "    lat = ds['latitude'].values   # (241,)\n",
    "\n",
    "    # Convert longitude from 0-360 to -180 to 180\n",
    "    lon2 = (lon + 180) % 360 - 180\n",
    "\n",
    "    # Swap the first and second halves\n",
    "    lon3 = np.copy(lon2)\n",
    "    lon3[:240] = lon2[240:480]\n",
    "    lon3[240:480] = lon2[:240]\n",
    "\n",
    "    # Create a meshgrid (not strictly needed for saving, but useful)\n",
    "    LON, LAT = np.meshgrid(lon3, lat)\n",
    "\n",
    "    # Apply the same transformation to co, no2, and go3\n",
    "    for var in [\"co\", \"no2\", \"no\", \"go3\"]:\n",
    "        if var in ds:\n",
    "            data = ds[var].values  # Shape: (time, lat, lon)\n",
    "            transformed_data = np.copy(data)\n",
    "\n",
    "            # Swap the longitude axis (last axis)\n",
    "            transformed_data[:, :, :240] = data[:, :, 240:480]\n",
    "            transformed_data[:, :, 240:480] = data[:, :, :240]\n",
    "\n",
    "            # Replace the dataset variable with the corrected data\n",
    "            ds[var].values = transformed_data\n",
    "\n",
    "    # Update longitude in the dataset\n",
    "    ds = ds.assign_coords(longitude=lon3)\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Saved transformed file: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc...\n",
      "Saved cropped file to D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc...\n",
      "Saved cropped file to D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# To crop to study area - DONE for 2003-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define your study area (lat_max, lon_min, lat_min, lon_max)\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Ensure latitude slicing is correct (since it decreases from 90 to -90)\n",
    "    ds_cropped = ds.sel(\n",
    "        latitude=slice(lat_max, lat_min),  # lat_max is greater than lat_min\n",
    "        longitude=slice(lon_min, lon_max)  # lon_min is less than lon_max\n",
    "    )\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the cropped dataset\n",
    "    ds_cropped.to_netcdf(output_file)\n",
    "    print(f\"Saved cropped file to {output_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Converted and saved: D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "✔ Converted and saved: D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n"
     ]
    }
   ],
   "source": [
    "# To convert from kg/kg to kg/m3 to ug/m3 (co in mg/m3) again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "chem_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\"\n",
    "temp_folder = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\3cropped_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "pressure = 1e5  # Pa equal to 1000 hPa\n",
    "R = 287.0500676  # J/(kg·K)\n",
    "\n",
    "# Loop through chemistry files\n",
    "for file in os.listdir(chem_folder):\n",
    "    if file.endswith('.nc'):\n",
    "        chem_path = os.path.join(chem_folder, file)\n",
    "\n",
    "        # Replace 'chem_multlvl' with 'meteo_multlvl' to find the corresponding temp file\n",
    "        temp_filename = file.replace('chem_multlvl', 'meteo_multlvl')\n",
    "        temp_path = os.path.join(temp_folder, temp_filename)\n",
    "\n",
    "        if not os.path.exists(temp_path):\n",
    "            print(f\"⚠ Temperature file not found for {file}\")\n",
    "            continue\n",
    "\n",
    "        # Open datasets\n",
    "        ds_chem = xr.open_dataset(chem_path)\n",
    "        ds_temp = xr.open_dataset(temp_path)\n",
    "\n",
    "        # Ensure 't' exists in temperature file\n",
    "        if 't' not in ds_temp.variables:\n",
    "            print(f\"⚠ No temperature variable 't' in {temp_filename}\")\n",
    "            continue\n",
    "\n",
    "        # Align time and space\n",
    "        ds_chem, ds_temp = xr.align(ds_chem, ds_temp, join='inner')\n",
    "        t = ds_temp['t']\n",
    "\n",
    "        # STEP 1: kg/kg → kg/m³\n",
    "        co_kgm3 = ds_chem['co'] * pressure / (R * t)\n",
    "        no2_kgm3 = ds_chem['no2'] * pressure / (R * t)\n",
    "        no_kgm3 = ds_chem['no'] * pressure / (R * t)\n",
    "        go3_kgm3 = ds_chem['go3'] * pressure / (R * t)\n",
    "\n",
    "        # STEP 2: kg/m³ → final units\n",
    "        co_final = co_kgm3 * 1e6    # mg/m³\n",
    "        no2_final = no2_kgm3 * 1e9  # µg/m³\n",
    "        no_final = no_kgm3 * 1e9    # µg/m³\n",
    "        go3_final = go3_kgm3 * 1e9  # µg/m³\n",
    "\n",
    "        # Create new dataset\n",
    "        new_ds = xr.Dataset({\n",
    "            'co': co_final,\n",
    "            'no2': no2_final,\n",
    "            'no': no_final,\n",
    "            'go3': go3_final\n",
    "        })\n",
    "\n",
    "        # Assign coordinates and attributes\n",
    "        for coord in ds_chem.coords:\n",
    "            new_ds = new_ds.assign_coords({coord: ds_chem[coord]})\n",
    "        new_ds.attrs = ds_chem.attrs\n",
    "\n",
    "        # Optional: add units metadata\n",
    "        new_ds['co'].attrs['units'] = 'mg m-3'\n",
    "        new_ds['no2'].attrs['units'] = 'µg m-3'\n",
    "        new_ds['no'].attrs['units'] = 'µg m-3'\n",
    "        new_ds['go3'].attrs['units'] = 'µg m-3'\n",
    "\n",
    "        # Save output\n",
    "        output_path = os.path.join(output_folder, f\"{file}\")\n",
    "        new_ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"✔ Converted and saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for 2003.\n",
      "No files found for 2004.\n",
      "No files found for 2005.\n",
      "No files found for 2006.\n",
      "No files found for 2007.\n",
      "No files found for 2008.\n",
      "No files found for 2009.\n",
      "No files found for 2010.\n",
      "No files found for 2011.\n",
      "No files found for 2012.\n",
      "No files found for 2013.\n",
      "No files found for 2014.\n",
      "No files found for 2015.\n",
      "No files found for 2016.\n",
      "No files found for 2017.\n",
      "No files found for 2018.\n",
      "No files found for 2019.\n",
      "No files found for 2020.\n",
      "No files found for 2021.\n",
      "No files found for 2022.\n",
      "No files found for 2023.\n",
      "Files for 2024 have been compiled into: D:\\IPMA\\CAMS\\chem_multlvl\\5compile_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_2024.nc\n"
     ]
    }
   ],
   "source": [
    "# To put together files for each year - DONE for 2003-2024 (final step) again\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the original path where the files are located\n",
    "input_path = r\"D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\"\n",
    "\n",
    "# Define the new path where you want to save the compiled files\n",
    "output_path_base = r\"D:\\IPMA\\CAMS\\chem_multlvl\\5compile_2003_2024\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_path_base, exist_ok=True)\n",
    "\n",
    "# Loop over each year from 2003 to 2024\n",
    "for year in range(2003, 2025):\n",
    "    # Create the file pattern for the specific year (e.g., 2003*)\n",
    "    file_pattern = os.path.join(input_path, f\"CAMS_global_reanalysis_EAC4_chem_multlvl_{year}*.nc\")\n",
    "    \n",
    "    # Use glob to find all .nc files for the specified year\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if files:  # Only proceed if there are files for that year\n",
    "        # Open all files for the year and concatenate along the 'time' dimension\n",
    "        ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "\n",
    "        # Save the compiled dataset to the new path\n",
    "        output_path = os.path.join(output_path_base, f\"CAMS_global_reanalysis_EAC4_chem_multlvl_{year}.nc\")\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Files for {year} have been compiled into: {output_path}\")\n",
    "    else:\n",
    "        print(f\"No files found for {year}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meteo multlvl - INTERMEDIATE STEP BEFORE RUNNING CONVERTION OF MULTLVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\CAMS\\meteo_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_meteo_multlvl_2024.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Iterate through years and months\n",
    "dest_folder = r\"E:\\IPMA\\CAMS\\meteo_multlvl\\0raw_2023_2024\"\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        # Filter dataset for the given year and month\n",
    "        monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "        if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "            output_filename = rf\"{dest_folder}\\CAMS_global_reanalysis_EAC4_meteo_multlvl_{year}{month:02d}.nc\"\n",
    "            monthly_ds.to_netcdf(output_filename)\n",
    "            print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change valid_time to time + remove pressure_level if present - DONE for 2023–2024 files again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\0raw_2023_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\1raw_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Process each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Rename 'valid_time' to 'time' if needed\n",
    "    if 'valid_time' in ds:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "    # Handle pressure_level if it's a dimension of size 1\n",
    "    if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n",
    "        ds = ds.isel(pressure_level=0).squeeze(drop=True)\n",
    "\n",
    "    # Drop pressure_level variable if it's still around (not a dimension)\n",
    "    if \"pressure_level\" in ds.variables and \"pressure_level\" not in ds.dims:\n",
    "        ds = ds.drop_vars(\"pressure_level\")\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    ds.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Saved cleaned file to: {output_file}\")\n",
    "\n",
    "print(\"✅ All files processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert coords from 0 to 360 to -180 to 180 - DONE for 2003-2024 again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\1raw_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\2transformation_2003_2024\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of NetCDF files in the input folder\n",
    "nc_files = [f for f in os.listdir(input_folder) if f.endswith(\".nc\")]\n",
    "\n",
    "# Process each file\n",
    "for nc_file in nc_files:\n",
    "    input_path = os.path.join(input_folder, nc_file)\n",
    "    output_path = os.path.join(output_folder, nc_file)\n",
    "\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Extract longitude and latitude\n",
    "    lon = ds['longitude'].values  # (480,)\n",
    "    lat = ds['latitude'].values   # (241,)\n",
    "\n",
    "    # Convert longitude from 0-360 to -180 to 180\n",
    "    lon2 = (lon + 180) % 360 - 180\n",
    "\n",
    "    # Swap the first and second halves\n",
    "    lon3 = np.copy(lon2)\n",
    "    lon3[:240] = lon2[240:480]\n",
    "    lon3[240:480] = lon2[:240]\n",
    "\n",
    "    # Create a meshgrid (not strictly needed for saving, but useful)\n",
    "    LON, LAT = np.meshgrid(lon3, lat)\n",
    "\n",
    "    # Apply the same transformation to co, no2, and go3\n",
    "    for var in [\"t\", \"q\"]:\n",
    "        if var in ds:\n",
    "            data = ds[var].values  # Shape: (time, lat, lon)\n",
    "            transformed_data = np.copy(data)\n",
    "\n",
    "            # Swap the longitude axis (last axis)\n",
    "            transformed_data[:, :, :240] = data[:, :, 240:480]\n",
    "            transformed_data[:, :, 240:480] = data[:, :, :240]\n",
    "\n",
    "            # Replace the dataset variable with the corrected data\n",
    "            ds[var].values = transformed_data\n",
    "\n",
    "    # Update longitude in the dataset\n",
    "    ds = ds.assign_coords(longitude=lon3)\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Saved transformed file: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - DONE for 2003-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define your study area (lat_max, lon_min, lat_min, lon_max)\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\2transformation_2003_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\3cropped_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Ensure latitude slicing is correct (since it decreases from 90 to -90)\n",
    "    ds_cropped = ds.sel(\n",
    "        latitude=slice(lat_max, lat_min),  # lat_max is greater than lat_min\n",
    "        longitude=slice(lon_min, lon_max)  # lon_min is less than lon_max\n",
    "    )\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the cropped dataset\n",
    "    ds_cropped.to_netcdf(output_file)\n",
    "    print(f\"Saved cropped file to {output_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put together all the csv files into one - DONE\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = r\"E:\\IPMA\\FRP\"\n",
    "\n",
    "# Get all CSV file paths from the folder\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# Read and concatenate the files\n",
    "df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset\n",
    "df.to_csv(os.path.join(folder_path, \"FRP_2001_2023.csv\"), index=False)\n",
    "\n",
    "print(\"All CSV files in the folder merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert .npy to .nc files - RAQUEL SOURCE - DONE\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Folder containing .npy files\n",
    "input_folder = r\"E:\\IPMA\\SPI\\SPI3\"\n",
    "output_folder = r\"E:\\IPMA\\SPI\\SPI3\\nc\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all .npy files in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".npy\"):  # Process only .npy files\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        data = np.load(file_path)  # Load the 3D array\n",
    "\n",
    "        # Define dimensions (adjust accordingly)\n",
    "        dims = (\"time\", \"lat\", \"lon\")  # Change based on data structure\n",
    "        coords = {\n",
    "            \"time\": np.arange(data.shape[0]),  # Modify based on actual data\n",
    "            \"lat\": np.linspace(34, 66, data.shape[1]),  # Modify latitudes\n",
    "            \"lon\": np.linspace(-12, 36, data.shape[2])  # Modify longitudes\n",
    "        }\n",
    "\n",
    "        # Convert to xarray DataArray\n",
    "        da = xr.DataArray(data, dims=dims, coords=coords, name=\"spi03\")\n",
    "\n",
    "        # Convert to Dataset\n",
    "        ds = da.to_dataset(name=\"spi03\")\n",
    "\n",
    "        # Save as NetCDF file\n",
    "        output_filename = filename.replace(\".npy\", \".nc\")\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Converted {filename} -> {output_filename}\")\n",
    "\n",
    "print(\"Batch conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate spei for 1979-2024 - WEB SOURCE - DONE\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "def extract_spei_data(input_file, start_year=1979, end_year=2024):\n",
    "    \"\"\"\n",
    "    Extracts data from a NetCDF file for the years between start_year and end_year and saves it to a new NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input NetCDF file.\n",
    "        start_year (int): Start year for extraction (default 1979).\n",
    "        end_year (int): End year for extraction (default 2024).\n",
    "    \"\"\"\n",
    "    # Open the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    # Ensure time is in datetime format (if it's not already in datetime format)\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "\n",
    "    # Filter the data based on the time dimension (between the start and end year)\n",
    "    filtered_ds = ds.sel(time=slice(f\"{start_year}-01-01\", f\"{end_year}-12-31\"))\n",
    "    \n",
    "    # Create output filename dynamically based on the input file name and year range\n",
    "    base_filename = input_file.split('/')[-1].split('.')[0]  # Extract file name (e.g., spei_01)\n",
    "    output_file = f\"{base_filename}_{start_year}-{end_year}.nc\"\n",
    "    \n",
    "    # Save the filtered dataset to a new NetCDF file\n",
    "    filtered_ds.to_netcdf(output_file)\n",
    "    print(f\"Saved filtered data to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\spei12.nc\"\n",
    "extract_spei_data(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate spei for each year between 1979-2024 - WEB SOURCE - DONE\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "def extract_spei_data_by_year(input_file, start_year=1979, end_year=2024):\n",
    "    \"\"\"\n",
    "    Extracts data from a NetCDF file for each year between start_year and end_year\n",
    "    and saves each year as a separate NetCDF file.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input NetCDF file.\n",
    "        start_year (int): Start year for extraction (default 1979).\n",
    "        end_year (int): End year for extraction (default 2024).\n",
    "    \"\"\"\n",
    "    # Open the NetCDF file using xarray\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    # Ensure time is in datetime format (if it's not already in datetime format)\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "\n",
    "    # Loop through each year and extract the data for that year\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Filter the dataset for the current year\n",
    "        filtered_ds = ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-12-31\"))\n",
    "        \n",
    "        # Create output filename for each year\n",
    "        base_filename = input_file.split('/')[-1].split('.')[0]  # Extract file name (e.g., spei_01)\n",
    "        output_file = f\"{base_filename}_{year}.nc\"\n",
    "        \n",
    "        # Save the filtered data for this year to a new NetCDF file\n",
    "        filtered_ds.to_netcdf(output_file)\n",
    "        print(f\"Saved filtered data for {year} to: {output_file}\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\spei12.nc\"\n",
    "extract_spei_data_by_year(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - WEB SOURCE - DONE for 1979-2023\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# Define your input and output directories\n",
    "input_dir = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\1raw_1979_2023\"\n",
    "output_dir = r\"E:\\IPMA\\SPEIbase_v2-10\\SPEI12\\2cropped_1979_2023\"\n",
    "\n",
    "# Define the latitude and longitude boundaries for your study area\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36  # Study area for Europe\n",
    "\n",
    "# Make sure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through all NC files in the input directory\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\".nc\"):  # Check if it's a NetCDF file\n",
    "        input_file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "        # Open the NetCDF file using xarray\n",
    "        with xr.open_dataset(input_file_path) as ds:\n",
    "            # Crop the dataset to include only the specified region\n",
    "            ds_europe = ds.sel(lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max))\n",
    "\n",
    "            # Create the output file path\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "            # Save the cropped data to a new NetCDF file\n",
    "            ds_europe.to_netcdf(output_file_path)\n",
    "            print(f\"Saved cropped file: {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
