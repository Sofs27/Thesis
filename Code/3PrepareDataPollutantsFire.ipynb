{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main pollutants PM10 & PM2.5, NO2, CO2, O3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMS global reanalysis EAC4 - Data Preparation (according to what Virgilio gave in matlab scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separates files by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\CAMS\\chem_singlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_singlvl_2024.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Iterate through years and months\n",
    "dest_folder = r\"E:\\IPMA\\CAMS\\chem_singlvl\\0raw_2023_2024\"\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        # Filter dataset for the given year and month\n",
    "        monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "        if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "            output_filename = rf\"{dest_folder}\\CAMS_global_reanalysis_EAC4_chem_singlvl_{year}{month:02d}.nc\"\n",
    "            monthly_ds.to_netcdf(output_filename)\n",
    "            print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes name variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change valid_time to time (so it matches) - DONE for 2023-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to your directory containing the NetCDF files\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\0raw_2023_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\1raw_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Loop through each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Check if 'valid_time' exists and rename it to 'time'\n",
    "    if 'valid_time' in ds:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "    # Get the filename and create the output path in the new directory\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the updated dataset to the output directory (overwrite original file in the new folder)\n",
    "    ds.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Renamed 'valid_time' to 'time' and saved to {output_file}\")\n",
    "\n",
    "print(\"Renaming and saving to new folder complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert coords from 0 to 360 to -180 to 180 - DONE for 2003-2024 again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\1raw_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\2transformation_2003_2024\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of NetCDF files in the input folder\n",
    "nc_files = [f for f in os.listdir(input_folder) if f.endswith(\".nc\")]\n",
    "\n",
    "# Process each file\n",
    "for nc_file in nc_files:\n",
    "    input_path = os.path.join(input_folder, nc_file)\n",
    "    output_path = os.path.join(output_folder, nc_file)\n",
    "\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Extract longitude and latitude\n",
    "    lon = ds['longitude'].values  # (480,)\n",
    "    lat = ds['latitude'].values   # (241,)\n",
    "\n",
    "    # Convert longitude from 0-360 to -180 to 180\n",
    "    lon2 = (lon + 180) % 360 - 180\n",
    "\n",
    "    # Swap the first and second halves\n",
    "    lon3 = np.copy(lon2)\n",
    "    lon3[:240] = lon2[240:480]\n",
    "    lon3[240:480] = lon2[:240]\n",
    "\n",
    "    # Create a meshgrid (not strictly needed for saving, but useful)\n",
    "    LON, LAT = np.meshgrid(lon3, lat)\n",
    "\n",
    "    # Apply the same transformation to pm10, pm1, and pm2p5\n",
    "    for var in [\"pm10\", \"pm1\", \"pm2p5\"]:\n",
    "        if var in ds:\n",
    "            data = ds[var].values  # Shape: (time, lat, lon)\n",
    "            transformed_data = np.copy(data)\n",
    "\n",
    "            # Swap the longitude axis (last axis)\n",
    "            transformed_data[:, :, :240] = data[:, :, 240:480]\n",
    "            transformed_data[:, :, 240:480] = data[:, :, :240]\n",
    "\n",
    "            # Replace the dataset variable with the corrected data\n",
    "            ds[var].values = transformed_data\n",
    "\n",
    "    # Update longitude in the dataset\n",
    "    ds = ds.assign_coords(longitude=lon3)\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Saved transformed file: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop to Study Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - DONE for 2003-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define your study area (lat_max, lon_min, lat_min, lon_max)\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\2transformation_2003_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_singlvl\\3cropped_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Ensure latitude slicing is correct (since it decreases from 90 to -90)\n",
    "    ds_cropped = ds.sel(\n",
    "        latitude=slice(lat_max, lat_min),  # lat_max is greater than lat_min\n",
    "        longitude=slice(lon_min, lon_max)  # lon_min is less than lon_max\n",
    "    )\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the cropped dataset\n",
    "    ds_cropped.to_netcdf(output_file)\n",
    "    print(f\"Saved cropped file to {output_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert PM from kg/m3 to ug/m3 - DONE again\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define input and output directories\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\3cropped_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_singlvl\\4conversion_2003_2024\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List of NetCDF files\n",
    "nc_files = glob.glob(os.path.join(input_folder, \"*.nc\"))\n",
    "\n",
    "for nc_file in nc_files:\n",
    "    try:\n",
    "        # Open the NetCDF file\n",
    "        ds = xr.open_dataset(nc_file)\n",
    "\n",
    "        # Convert PM values to µg/m³\n",
    "        ds[\"pm1\"] = ds[\"pm1\"] * 1e9\n",
    "        ds[\"pm2p5\"] = ds[\"pm2p5\"] * 1e9\n",
    "        ds[\"pm10\"] = ds[\"pm10\"] * 1e9\n",
    "\n",
    "        # Get the filename and construct output path\n",
    "        filename = os.path.basename(nc_file)  # Extracts filename only\n",
    "        output_file = os.path.join(output_folder, filename)  # Keeps same filename in new folder\n",
    "\n",
    "        # Save the modified NetCDF file in the new folder\n",
    "        ds.to_netcdf(output_file)\n",
    "        ds.close()  # Close the dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {nc_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puts together files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To put together files for each year - DONE for 2003-2024 (final step) again\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the original path where the files are located\n",
    "input_path = r\"D:\\IPMA\\CAMS\\chem_singlvl\\4conversion_2003_2024\"\n",
    "\n",
    "# Define the new path where you want to save the compiled files\n",
    "output_path_base = r\"D:\\IPMA\\CAMS\\chem_singlvl\\5compile_2003_2024\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_path_base, exist_ok=True)\n",
    "\n",
    "# Loop over each year from 2003 to 2024\n",
    "for year in range(2003, 2025):\n",
    "    # Create the file pattern for the specific year (e.g., 2003*)\n",
    "    file_pattern = os.path.join(input_path, f\"CAMS_global_reanalysis_EAC4_chem_singlvl_{year}*.nc\")\n",
    "    \n",
    "    # Use glob to find all .nc files for the specified year\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if files:  # Only proceed if there are files for that year\n",
    "        # Open all files for the year and concatenate along the 'time' dimension\n",
    "        ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "\n",
    "        # Save the compiled dataset to the new path\n",
    "        output_path = os.path.join(output_path_base, f\"CAMS_global_reanalysis_EAC4_chem_singlvl_{year}.nc\")\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Files for {year} have been compiled into: {output_path}\")\n",
    "    else:\n",
    "        print(f\"No files found for {year}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separates files by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_2024.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Iterate through years and months\n",
    "dest_folder = r\"E:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\"\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        # Filter dataset for the given year and month\n",
    "        monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "        if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "            output_filename = rf\"{dest_folder}\\CAMS_global_reanalysis_EAC4_chem_multlvl_{year}{month:02d}.nc\"\n",
    "            monthly_ds.to_netcdf(output_filename)\n",
    "            print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes name variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15100\\494470751.py:29: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file to: D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_15100\\494470751.py:29: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned file to: D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n",
      "✅ All files processed!\n"
     ]
    }
   ],
   "source": [
    "# To change valid_time to time + remove pressure_level if present - DONE for 2023–2024 files again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\0raw_2023_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Process each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Rename 'valid_time' to 'time' if needed\n",
    "    if 'valid_time' in ds:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "    # Handle pressure_level if it's a dimension of size 1\n",
    "    if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n",
    "        ds = ds.isel(pressure_level=0).squeeze(drop=True)\n",
    "\n",
    "    # Drop pressure_level variable if it's still around (not a dimension)\n",
    "    if \"pressure_level\" in ds.variables and \"pressure_level\" not in ds.dims:\n",
    "        ds = ds.drop_vars(\"pressure_level\")\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    ds.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Saved cleaned file to: {output_file}\")\n",
    "\n",
    "print(\"✅ All files processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Saved transformed file: D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Processing: CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n",
      "Saved transformed file: D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n"
     ]
    }
   ],
   "source": [
    "# To convert coords from 0 to 360 to -180 to 180 - DONE for 2003-2024 again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\1raw_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of NetCDF files in the input folder\n",
    "nc_files = [f for f in os.listdir(input_folder) if f.endswith(\".nc\")]\n",
    "\n",
    "# Process each file\n",
    "for nc_file in nc_files:\n",
    "    input_path = os.path.join(input_folder, nc_file)\n",
    "    output_path = os.path.join(output_folder, nc_file)\n",
    "\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Extract longitude and latitude\n",
    "    lon = ds['longitude'].values  # (480,)\n",
    "    lat = ds['latitude'].values   # (241,)\n",
    "\n",
    "    # Convert longitude from 0-360 to -180 to 180\n",
    "    lon2 = (lon + 180) % 360 - 180\n",
    "\n",
    "    # Swap the first and second halves\n",
    "    lon3 = np.copy(lon2)\n",
    "    lon3[:240] = lon2[240:480]\n",
    "    lon3[240:480] = lon2[:240]\n",
    "\n",
    "    # Create a meshgrid (not strictly needed for saving, but useful)\n",
    "    LON, LAT = np.meshgrid(lon3, lat)\n",
    "\n",
    "    # Apply the same transformation to co, no2, and go3\n",
    "    for var in [\"co\", \"no2\", \"no\", \"go3\"]:\n",
    "        if var in ds:\n",
    "            data = ds[var].values  # Shape: (time, lat, lon)\n",
    "            transformed_data = np.copy(data)\n",
    "\n",
    "            # Swap the longitude axis (last axis)\n",
    "            transformed_data[:, :, :240] = data[:, :, 240:480]\n",
    "            transformed_data[:, :, 240:480] = data[:, :, :240]\n",
    "\n",
    "            # Replace the dataset variable with the corrected data\n",
    "            ds[var].values = transformed_data\n",
    "\n",
    "    # Update longitude in the dataset\n",
    "    ds = ds.assign_coords(longitude=lon3)\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Saved transformed file: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop to study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc...\n",
      "Saved cropped file to D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "Processing D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc...\n",
      "Saved cropped file to D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# To crop to study area - DONE for 2003-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define your study area (lat_max, lon_min, lat_min, lon_max)\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\2transformation_2003_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Ensure latitude slicing is correct (since it decreases from 90 to -90)\n",
    "    ds_cropped = ds.sel(\n",
    "        latitude=slice(lat_max, lat_min),  # lat_max is greater than lat_min\n",
    "        longitude=slice(lon_min, lon_max)  # lon_min is less than lon_max\n",
    "    )\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the cropped dataset\n",
    "    ds_cropped.to_netcdf(output_file)\n",
    "    print(f\"Saved cropped file to {output_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Converted and saved: D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202411.nc\n",
      "✔ Converted and saved: D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_202412.nc\n"
     ]
    }
   ],
   "source": [
    "# To convert from kg/kg to kg/m3 to ug/m3 (co in mg/m3) again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "chem_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\3cropped_2003_2024\"\n",
    "temp_folder = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\3cropped_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Constants\n",
    "pressure = 1e5  # Pa equal to 1000 hPa\n",
    "R = 287.0500676  # J/(kg·K)\n",
    "\n",
    "# Loop through chemistry files\n",
    "for file in os.listdir(chem_folder):\n",
    "    if file.endswith('.nc'):\n",
    "        chem_path = os.path.join(chem_folder, file)\n",
    "\n",
    "        # Replace 'chem_multlvl' with 'meteo_multlvl' to find the corresponding temp file\n",
    "        temp_filename = file.replace('chem_multlvl', 'meteo_multlvl')\n",
    "        temp_path = os.path.join(temp_folder, temp_filename)\n",
    "\n",
    "        if not os.path.exists(temp_path):\n",
    "            print(f\"⚠ Temperature file not found for {file}\")\n",
    "            continue\n",
    "\n",
    "        # Open datasets\n",
    "        ds_chem = xr.open_dataset(chem_path)\n",
    "        ds_temp = xr.open_dataset(temp_path)\n",
    "\n",
    "        # Ensure 't' exists in temperature file\n",
    "        if 't' not in ds_temp.variables:\n",
    "            print(f\"⚠ No temperature variable 't' in {temp_filename}\")\n",
    "            continue\n",
    "\n",
    "        # Align time and space\n",
    "        ds_chem, ds_temp = xr.align(ds_chem, ds_temp, join='inner')\n",
    "        t = ds_temp['t']\n",
    "\n",
    "        # STEP 1: kg/kg → kg/m³\n",
    "        co_kgm3 = ds_chem['co'] * pressure / (R * t)\n",
    "        no2_kgm3 = ds_chem['no2'] * pressure / (R * t)\n",
    "        no_kgm3 = ds_chem['no'] * pressure / (R * t)\n",
    "        go3_kgm3 = ds_chem['go3'] * pressure / (R * t)\n",
    "\n",
    "        # STEP 2: kg/m³ → final units\n",
    "        co_final = co_kgm3 * 1e6    # mg/m³\n",
    "        no2_final = no2_kgm3 * 1e9  # µg/m³\n",
    "        no_final = no_kgm3 * 1e9    # µg/m³\n",
    "        go3_final = go3_kgm3 * 1e9  # µg/m³\n",
    "\n",
    "        # Create new dataset\n",
    "        new_ds = xr.Dataset({\n",
    "            'co': co_final,\n",
    "            'no2': no2_final,\n",
    "            'no': no_final,\n",
    "            'go3': go3_final\n",
    "        })\n",
    "\n",
    "        # Assign coordinates and attributes\n",
    "        for coord in ds_chem.coords:\n",
    "            new_ds = new_ds.assign_coords({coord: ds_chem[coord]})\n",
    "        new_ds.attrs = ds_chem.attrs\n",
    "\n",
    "        # Optional: add units metadata\n",
    "        new_ds['co'].attrs['units'] = 'mg m-3'\n",
    "        new_ds['no2'].attrs['units'] = 'µg m-3'\n",
    "        new_ds['no'].attrs['units'] = 'µg m-3'\n",
    "        new_ds['go3'].attrs['units'] = 'µg m-3'\n",
    "\n",
    "        # Save output\n",
    "        output_path = os.path.join(output_folder, f\"{file}\")\n",
    "        new_ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"✔ Converted and saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puts together files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for 2003.\n",
      "No files found for 2004.\n",
      "No files found for 2005.\n",
      "No files found for 2006.\n",
      "No files found for 2007.\n",
      "No files found for 2008.\n",
      "No files found for 2009.\n",
      "No files found for 2010.\n",
      "No files found for 2011.\n",
      "No files found for 2012.\n",
      "No files found for 2013.\n",
      "No files found for 2014.\n",
      "No files found for 2015.\n",
      "No files found for 2016.\n",
      "No files found for 2017.\n",
      "No files found for 2018.\n",
      "No files found for 2019.\n",
      "No files found for 2020.\n",
      "No files found for 2021.\n",
      "No files found for 2022.\n",
      "No files found for 2023.\n",
      "Files for 2024 have been compiled into: D:\\IPMA\\CAMS\\chem_multlvl\\5compile_2003_2024\\CAMS_global_reanalysis_EAC4_chem_multlvl_2024.nc\n"
     ]
    }
   ],
   "source": [
    "# To put together files for each year - DONE for 2003-2024 (final step) again\n",
    "\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the original path where the files are located\n",
    "input_path = r\"D:\\IPMA\\CAMS\\chem_multlvl\\4conversion_2003_2024\"\n",
    "\n",
    "# Define the new path where you want to save the compiled files\n",
    "output_path_base = r\"D:\\IPMA\\CAMS\\chem_multlvl\\5compile_2003_2024\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_path_base, exist_ok=True)\n",
    "\n",
    "# Loop over each year from 2003 to 2024\n",
    "for year in range(2003, 2025):\n",
    "    # Create the file pattern for the specific year (e.g., 2003*)\n",
    "    file_pattern = os.path.join(input_path, f\"CAMS_global_reanalysis_EAC4_chem_multlvl_{year}*.nc\")\n",
    "    \n",
    "    # Use glob to find all .nc files for the specified year\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if files:  # Only proceed if there are files for that year\n",
    "        # Open all files for the year and concatenate along the 'time' dimension\n",
    "        ds = xr.open_mfdataset(files, combine='by_coords')\n",
    "\n",
    "        # Save the compiled dataset to the new path\n",
    "        output_path = os.path.join(output_path_base, f\"CAMS_global_reanalysis_EAC4_chem_multlvl_{year}.nc\")\n",
    "        ds.to_netcdf(output_path)\n",
    "\n",
    "        print(f\"Files for {year} have been compiled into: {output_path}\")\n",
    "    else:\n",
    "        print(f\"No files found for {year}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meteo multlvl - INTERMEDIATE STEP BEFORE RUNNING CONVERTION OF MULTLVL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separates files by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate monthly files when downloading the full year - DONE for 2023-2024 nov\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:\\IPMA\\CAMS\\meteo_multlvl\\0raw_2023_2024\\CAMS_global_reanalysis_EAC4_meteo_multlvl_2024.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# Ensure valid_time is a datetime object\n",
    "ds['valid_time'] = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "# Get unique years in the dataset\n",
    "years = pd.Series(ds['valid_time'].dt.year.values).unique()\n",
    "\n",
    "# Iterate through years and months\n",
    "dest_folder = r\"E:\\IPMA\\CAMS\\meteo_multlvl\\0raw_2023_2024\"\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        # Filter dataset for the given year and month\n",
    "        monthly_ds = ds.sel(valid_time=(ds.valid_time.dt.year == year) & (ds.valid_time.dt.month == month))\n",
    "\n",
    "        if monthly_ds.valid_time.size > 0:  # Only save if data exists for the month\n",
    "            output_filename = rf\"{dest_folder}\\CAMS_global_reanalysis_EAC4_meteo_multlvl_{year}{month:02d}.nc\"\n",
    "            monthly_ds.to_netcdf(output_filename)\n",
    "            print(f\"Saved {output_filename}\")\n",
    "\n",
    "# Close the dataset\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes name variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To change valid_time to time + remove pressure_level if present - DONE for 2023–2024 files again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\0raw_2023_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\1raw_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "# Process each file\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Rename 'valid_time' to 'time' if needed\n",
    "    if 'valid_time' in ds:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "\n",
    "    # Handle pressure_level if it's a dimension of size 1\n",
    "    if \"pressure_level\" in ds.dims and ds.dims[\"pressure_level\"] == 1:\n",
    "        ds = ds.isel(pressure_level=0).squeeze(drop=True)\n",
    "\n",
    "    # Drop pressure_level variable if it's still around (not a dimension)\n",
    "    if \"pressure_level\" in ds.variables and \"pressure_level\" not in ds.dims:\n",
    "        ds = ds.drop_vars(\"pressure_level\")\n",
    "\n",
    "    # Save cleaned dataset\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    ds.to_netcdf(output_file)\n",
    "\n",
    "    print(f\"Saved cleaned file to: {output_file}\")\n",
    "\n",
    "print(\"✅ All files processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert coords from 0 to 360 to -180 to 180 - DONE for 2003-2024 again\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\1raw_2003_2024\"\n",
    "output_folder = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\2transformation_2003_2024\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get list of NetCDF files in the input folder\n",
    "nc_files = [f for f in os.listdir(input_folder) if f.endswith(\".nc\")]\n",
    "\n",
    "# Process each file\n",
    "for nc_file in nc_files:\n",
    "    input_path = os.path.join(input_folder, nc_file)\n",
    "    output_path = os.path.join(output_folder, nc_file)\n",
    "\n",
    "    print(f\"Processing: {nc_file}\")\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(input_path)\n",
    "\n",
    "    # Extract longitude and latitude\n",
    "    lon = ds['longitude'].values  # (480,)\n",
    "    lat = ds['latitude'].values   # (241,)\n",
    "\n",
    "    # Convert longitude from 0-360 to -180 to 180\n",
    "    lon2 = (lon + 180) % 360 - 180\n",
    "\n",
    "    # Swap the first and second halves\n",
    "    lon3 = np.copy(lon2)\n",
    "    lon3[:240] = lon2[240:480]\n",
    "    lon3[240:480] = lon2[:240]\n",
    "\n",
    "    # Create a meshgrid (not strictly needed for saving, but useful)\n",
    "    LON, LAT = np.meshgrid(lon3, lat)\n",
    "\n",
    "    # Apply the same transformation to co, no2, and go3\n",
    "    for var in [\"t\", \"q\"]:\n",
    "        if var in ds:\n",
    "            data = ds[var].values  # Shape: (time, lat, lon)\n",
    "            transformed_data = np.copy(data)\n",
    "\n",
    "            # Swap the longitude axis (last axis)\n",
    "            transformed_data[:, :, :240] = data[:, :, 240:480]\n",
    "            transformed_data[:, :, 240:480] = data[:, :, :240]\n",
    "\n",
    "            # Replace the dataset variable with the corrected data\n",
    "            ds[var].values = transformed_data\n",
    "\n",
    "    # Update longitude in the dataset\n",
    "    ds = ds.assign_coords(longitude=lon3)\n",
    "\n",
    "    # Save the modified dataset\n",
    "    ds.to_netcdf(output_path)\n",
    "\n",
    "    print(f\"Saved transformed file: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop to study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To crop to study area - DONE for 2003-2024 again\n",
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define your study area (lat_max, lon_min, lat_min, lon_max)\n",
    "lat_max, lon_min, lat_min, lon_max = 66, -12, 34, 36\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\2transformation_2003_2024\"\n",
    "output_dir = r\"D:\\IPMA\\CAMS\\meteo_multlvl\\3cropped_2003_2024\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all NetCDF files in the input directory\n",
    "nc_files = glob.glob(os.path.join(input_dir, \"*.nc\"))\n",
    "\n",
    "for file_path in nc_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Ensure latitude slicing is correct (since it decreases from 90 to -90)\n",
    "    ds_cropped = ds.sel(\n",
    "        latitude=slice(lat_max, lat_min),  # lat_max is greater than lat_min\n",
    "        longitude=slice(lon_min, lon_max)  # lon_min is less than lon_max\n",
    "    )\n",
    "\n",
    "    # Define output file path\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Save the cropped dataset\n",
    "    ds_cropped.to_netcdf(output_file)\n",
    "    print(f\"Saved cropped file to {output_file}\")\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
